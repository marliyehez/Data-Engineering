{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "85V5_vg0XZbo",
        "ruZWPFW8YiSG",
        "FffpiqXPYrX8",
        "fateCCdVY9Oj",
        "J_m5b_FEY-ha",
        "hoSDMkeJYxjW",
        "yvBUxcKhZKwR",
        "yoYwtck_ZOPD",
        "kZFMITSKZOLr",
        "u9QgGwujZOAT",
        "meDz1I5MZmaz",
        "BkajnZEIZbJa",
        "PpMyYYsjZOSe",
        "YY6vS_92aAy8",
        "pGcqFYehbFCv",
        "q6xrko3HbGht",
        "jRLYer72a4i5",
        "n7Av2rJoa10G",
        "8JMmDJSZaCNz",
        "whd_6bXhbMRo",
        "4Rg6lMaaaCG_",
        "omUNxRonbovi",
        "3zseH-amaCtd",
        "Fma0x54MbzLc",
        "fCEM7GnMb1SA",
        "TxtSWHkMb39B",
        "KnBEI01mcKML",
        "X2AkgtqAgYj0",
        "H5BJFlj8gYcW"
      ],
      "authorship_tag": "ABX9TyMr8EjmhPEjMZBPxxo64gWO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marliyehez/Data-Engineering/blob/main/pyspark_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "qSvyl5UASFV4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "4ZbQJvyEXZ1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic"
      ],
      "metadata": {
        "id": "DhDl5l6Ms6A0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5ha9y50Ho5v",
        "outputId": "bc763faf-b553-49a2-cc86-bd0095df4211"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading titanic.zip to /content\n",
            "  0% 0.00/34.1k [00:00<?, ?B/s]\n",
            "100% 34.1k/34.1k [00:00<00:00, 33.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change kaggle config dir\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "\n",
        "# download the data via api\n",
        "!kaggle competitions download -c titanic\n",
        "\n",
        "# unzip the data\n",
        "!unzip titanic.zip"
      ],
      "metadata": {
        "id": "YLPFpUYxtLG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Kaggle Titanic') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Spark read\n",
        "csv_path = '/content/train.csv'\n",
        "df = spark.read.csv(csv_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "7QukPFCgsrnV"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_oI_rflH5JF",
        "outputId": "f70067b7-16a7-4c89-c154-1dd795dfa7f2"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.approxQuantile('Age', [0.5], 0.01)[0]\n",
        "df.approxQuantile('SibSp', [0.5], 0.01)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4XBviC1Ints",
        "outputId": "7de9cb29-ffb7-41eb-ef61-0c0e84aeb99f"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqEc0ybR5rNj",
        "outputId": "bfe27d7e-4dad-4c7e-dd52-222cb5211a5c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Height: string (nullable = true)\n",
            " |-- Weight: string (nullable = true)\n",
            " |-- Team: string (nullable = true)\n",
            " |-- NOC: string (nullable = true)\n",
            " |-- Games: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Season: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Sport: string (nullable = true)\n",
            " |-- Event: string (nullable = true)\n",
            " |-- Medal: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change NA into DNW (Do Not Win)\n",
        "df = df.withColumn(\"Medal\", F.when(F.col(\"Medal\") == \"NA\", \"DNW\").otherwise(F.col(\"Medal\"))).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npMrgSLwFFpr",
        "outputId": "72e84f61-343d-4fb6-a204-39a1d6e9cd14"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+-----------+-------------+--------------------+-----+\n",
            "| ID|                Name|Sex|Age|Height|Weight|          Team|NOC|      Games|Year|Season|       City|        Sport|               Event|Medal|\n",
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+-----------+-------------+--------------------+-----+\n",
            "|  1|           A Dijiang|  M| 24| 180.0|  80.0|         China|CHN|1992 Summer|1992|Summer|  Barcelona|   Basketball|Basketball Men's ...|  DNW|\n",
            "|  2|            A Lamusi|  M| 23| 170.0|  60.0|         China|CHN|2012 Summer|2012|Summer|     London|         Judo|Judo Men's Extra-...|  DNW|\n",
            "|  3| Gunnar Nielsen Aaby|  M| 24|  null|  null|       Denmark|DEN|1920 Summer|1920|Summer|  Antwerpen|     Football|Football Men's Fo...|  DNW|\n",
            "|  4|Edgar Lindenau Aabye|  M| 34|  null|  null|Denmark/Sweden|DEN|1900 Summer|1900|Summer|      Paris|   Tug-Of-War|Tug-Of-War Men's ...| Gold|\n",
            "|  5|Christine Jacoba ...|  F| 21| 185.0|  82.0|   Netherlands|NED|1988 Winter|1988|Winter|    Calgary|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 21| 185.0|  82.0|   Netherlands|NED|1988 Winter|1988|Winter|    Calgary|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 25| 185.0|  82.0|   Netherlands|NED|1992 Winter|1992|Winter|Albertville|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 25| 185.0|  82.0|   Netherlands|NED|1992 Winter|1992|Winter|Albertville|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 27| 185.0|  82.0|   Netherlands|NED|1994 Winter|1994|Winter|Lillehammer|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 27| 185.0|  82.0|   Netherlands|NED|1994 Winter|1994|Winter|Lillehammer|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+-----------+-------------+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LO72csgFK67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\n",
        "    [F.sum(F.col(c).isNull().cast('int')).alias(c) for c in df.columns]\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR4FbCGmyjJ_",
        "outputId": "18e64082-16e4-4fc6-b8ac-48b6ae7f7634"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+--------+--------+-----------+-----------+---------+--------+----------+---------+-----------+---------+----------+----------+----------+\n",
            "|IDCount|NameCount|SexCount|AgeCount|HeightCount|WeightCount|TeamCount|NOCCount|GamesCount|YearCount|SeasonCount|CityCount|SportCount|EventCount|MedalCount|\n",
            "+-------+---------+--------+--------+-----------+-----------+---------+--------+----------+---------+-----------+---------+----------+----------+----------+\n",
            "|      0|        0|       0|       0|          0|          0|        0|       0|         0|        0|          0|        0|         0|         0|         0|\n",
            "+-------+---------+--------+--------+-----------+-----------+---------+--------+----------+---------+-----------+---------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQMs4j5D0zbg",
        "outputId": "3f71f8be-7fb1-40fb-9ac8-ecaaf4d40b77"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+-----+------+------+----+---+-----+----+------+----+-----+-----+-----+\n",
            "| ID|Name|Sex|  Age|Height|Weight|Team|NOC|Games|Year|Season|City|Sport|Event|Medal|\n",
            "+---+----+---+-----+------+------+----+---+-----+----+------+----+-----+-----+-----+\n",
            "|  0|   0|  0|10145| 60006| 62857|   0|  0|    0| 673|     0|   0|    0|    0|    0|\n",
            "+---+----+---+-----+------+------+----+---+-----+----+------+----+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJitueBWEYBq",
        "outputId": "6c899c98-ce20-4706-ef18-ec75ed1c40c7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+-----------+-------------+--------------------+-----+\n",
            "| ID|                Name|Sex|Age|Height|Weight|          Team|NOC|      Games|Year|Season|       City|        Sport|               Event|Medal|\n",
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+-----------+-------------+--------------------+-----+\n",
            "|  1|           A Dijiang|  M| 24| 180.0|  80.0|         China|CHN|1992 Summer|1992|Summer|  Barcelona|   Basketball|Basketball Men's ...|  DNW|\n",
            "|  2|            A Lamusi|  M| 23| 170.0|  60.0|         China|CHN|2012 Summer|2012|Summer|     London|         Judo|Judo Men's Extra-...|  DNW|\n",
            "|  3| Gunnar Nielsen Aaby|  M| 24|  null|  null|       Denmark|DEN|1920 Summer|1920|Summer|  Antwerpen|     Football|Football Men's Fo...|  DNW|\n",
            "|  4|Edgar Lindenau Aabye|  M| 34|  null|  null|Denmark/Sweden|DEN|1900 Summer|1900|Summer|      Paris|   Tug-Of-War|Tug-Of-War Men's ...| Gold|\n",
            "|  5|Christine Jacoba ...|  F| 21| 185.0|  82.0|   Netherlands|NED|1988 Winter|1988|Winter|    Calgary|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 21| 185.0|  82.0|   Netherlands|NED|1988 Winter|1988|Winter|    Calgary|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 25| 185.0|  82.0|   Netherlands|NED|1992 Winter|1992|Winter|Albertville|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 25| 185.0|  82.0|   Netherlands|NED|1992 Winter|1992|Winter|Albertville|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 27| 185.0|  82.0|   Netherlands|NED|1994 Winter|1994|Winter|Lillehammer|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "|  5|Christine Jacoba ...|  F| 27| 185.0|  82.0|   Netherlands|NED|1994 Winter|1994|Winter|Lillehammer|Speed Skating|Speed Skating Wom...|  DNW|\n",
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+-----------+-------------+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_hyFqY1vVqD",
        "outputId": "6a63ce4d-0068-4238-8824-315cd3486e37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+---------+-------------+--------------------+-----+\n",
            "| ID|                Name|Sex|Age|Height|Weight|          Team|NOC|      Games|Year|Season|     City|        Sport|               Event|Medal|\n",
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+---------+-------------+--------------------+-----+\n",
            "|  1|           A Dijiang|  M| 24|   180|    80|         China|CHN|1992 Summer|1992|Summer|Barcelona|   Basketball|Basketball Men's ...|   NA|\n",
            "|  2|            A Lamusi|  M| 23|   170|    60|         China|CHN|2012 Summer|2012|Summer|   London|         Judo|Judo Men's Extra-...|   NA|\n",
            "|  3| Gunnar Nielsen Aaby|  M| 24|    NA|    NA|       Denmark|DEN|1920 Summer|1920|Summer|Antwerpen|     Football|Football Men's Fo...|   NA|\n",
            "|  4|Edgar Lindenau Aabye|  M| 34|    NA|    NA|Denmark/Sweden|DEN|1900 Summer|1900|Summer|    Paris|   Tug-Of-War|Tug-Of-War Men's ...| Gold|\n",
            "|  5|Christine Jacoba ...|  F| 21|   185|    82|   Netherlands|NED|1988 Winter|1988|Winter|  Calgary|Speed Skating|Speed Skating Wom...|   NA|\n",
            "+---+--------------------+---+---+------+------+--------------+---+-----------+----+------+---------+-------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3IqXewKIyVvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transjakarta"
      ],
      "metadata": {
        "id": "OAEGCNmYWwit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery"
      ],
      "metadata": {
        "id": "po6gyoOwHw2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, concat, concat_ws, create_map, lit\n",
        "from itertools import chain\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Transjakarta ETL') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set the Google Cloud project ID\n",
        "project_id = \"my-project-390806\"\n",
        "spark.conf.set(\"spark.datasource.bigquery.projectId\", project_id)\n",
        "\n",
        "# Set the Google Cloud service account key file path\n",
        "spark.conf.set(\"spark.datasource.bigquery.keyfile\", \"/content/my-project-390806-8bdde486c5da.json\")\n",
        "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\", \"true\")\n",
        "\n",
        "# Get csv files\n",
        "import os\n",
        "directory = \"./data\"\n",
        "csv_files = [filename for filename in os.listdir(directory) if filename.endswith('.csv')]\n",
        "\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through the CSV files and read them into DataFrames\n",
        "for filename in csv_files:\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Merge the DataFrames using union()\n",
        "fact_table = dataframes[0]\n",
        "for df in dataframes[1:]:\n",
        "    fact_table = fact_table.union(df)\n",
        "\n",
        "# Sort based on date\n",
        "fact_table = fact_table.orderBy(col(\"tahun\").asc(), col(\"bulan\").asc())\n",
        "\n",
        "# Dimension tables\n",
        "date_dim = fact_table.select(\"bulan\", \"tahun\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"tahun\").asc(), col(\"bulan\").asc()) \\\n",
        "    .withColumn(\"id\", monotonically_increasing_id()) \\\n",
        "    .select(\"id\", \"bulan\", \"tahun\")\n",
        "\n",
        "route_dim = fact_table.select(\"kode_trayek\", \"trayek\", \"jenis\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .withColumn(\"id\", monotonically_increasing_id()) \\\n",
        "    .select(\"id\", \"kode_trayek\", \"trayek\", \"jenis\")\n",
        "\n",
        "vehicle_dim = route_dim.select(\"jenis\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .withColumn(\"id\", monotonically_increasing_id()) \\\n",
        "    .select(\"id\", \"jenis\")\n",
        "\n",
        "\n",
        "# Foreign keys dict\n",
        "vehicle_dict = vehicle_dim.select(\"jenis\", \"id\").rdd.collectAsMap()\n",
        "route_dict = route_dim.select(\"kode_trayek\", \"id\").rdd.collectAsMap()\n",
        "date_dict = date_dim.withColumn(\n",
        "    \"bulan_tahun\",\n",
        "    concat_ws(\n",
        "        \"_\",\n",
        "        col(\"bulan\"),\n",
        "        col(\"tahun\")\n",
        "    )\n",
        ").select(\"bulan_tahun\", \"id\").rdd.collectAsMap()\n",
        "\n",
        "# Creating foregin keys\n",
        "mapping_expr = create_map([lit(x) for x in chain(*vehicle_dict.items())])\n",
        "route_dim = route_dim.withColumn(\"vehicle_fk\", mapping_expr.__getitem__(col(\"jenis\")))\n",
        "\n",
        "mapping_expr = create_map([lit(x) for x in chain(*route_dict.items())])\n",
        "fact_table = fact_table.withColumn(\"route_fk\", mapping_expr.__getitem__(col(\"kode_trayek\")))\n",
        "\n",
        "mapping_expr = create_map([lit(x) for x in chain(*date_dict.items())])\n",
        "fact_table = fact_table.withColumn(\n",
        "    \"bulan_tahun\",\n",
        "    concat_ws(\n",
        "        \"_\",\n",
        "        col(\"bulan\"),\n",
        "        col(\"tahun\")\n",
        "    )\n",
        ").withColumn(\"date_fk\", mapping_expr.__getitem__(col(\"bulan_tahun\")))\n",
        "\n",
        "\n",
        "# Drop columns\n",
        "cols = [\"tahun\", \"bulan\", \"jenis\", \"kode_trayek\", \"trayek\", \"bulan_tahun\"]\n",
        "fact_table = fact_table.drop(*cols)\n",
        "route_dim = route_dim.drop(\"jenis\")\n",
        "\n",
        "\n",
        "# Define the BigQuery dataset and table name\n",
        "dataset_name = \"transjakarta\"\n",
        "tables_list = [\n",
        "    (fact_table,'fact table'),\n",
        "    (route_dim, 'route dim'),\n",
        "    (date_dim, 'date dim'),\n",
        "    (vehicle_dim, 'vehicle dim'),\n",
        "]"
      ],
      "metadata": {
        "id": "kEw6iDk8WzeQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "998fef27-99c9-4c18-f3a5-7229f5f9fdb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-feb99acac504>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Set the Google Cloud service account key file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.datasource.bigquery.keyfile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/my-project-390806-8bdde486c5da.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Get csv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/conf.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mSupports\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0mConnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: The SQL config 'spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation' was removed in the version 3.0.0. It was removed to prevent loss of user data for non-default value."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the DataFrame to BigQuery\n",
        "try:\n",
        "\n",
        "    for df, table_name in tables_list:\n",
        "        table_name = 'fact table'\n",
        "        fact_table.write \\\n",
        "            .format(\"bigquery\") \\\n",
        "            .option(\"table\", f\"{project_id}.{dataset_name}.{table_name}\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save()\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "GmI5cg0_GcEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GCS CODE\n",
        "\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, concat, concat_ws, create_map, lit\n",
        "from itertools import chain\n",
        "import gcsfs\n",
        "\n",
        "\n",
        "# Set the GCS bucket and folder path\n",
        "BUCKET = \"transjakarta-data\"\n",
        "FOLDER_PATH = \"raw-data\"\n",
        "\n",
        "# Create a file system object\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "# List all the csv files in the folder\n",
        "files = fs.ls(f\"{BUCKET}/{FOLDER_PATH}\")\n",
        "csv_files = [f\"gs://{file}\" for file in files if file.endswith('.csv')]\n",
        "\n",
        "\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through the CSV files and read them into DataFrames\n",
        "for filename in csv_files:\n",
        "    df = spark.read.csv(filename, header=True, inferSchema=True)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Merge the DataFrames using union()\n",
        "fact_table = dataframes[0]\n",
        "for df in dataframes[1:]:\n",
        "    fact_table = fact_table.union(df)\n",
        "\n",
        "# Sort based on date\n",
        "fact_table = fact_table.orderBy(col(\"tahun\").asc(), col(\"bulan\").asc())\n",
        "\n",
        "# Dimension tables\n",
        "date_dim = fact_table.select(\"bulan\", \"tahun\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .orderBy(col(\"tahun\").asc(), col(\"bulan\").asc()) \\\n",
        "    .withColumn(\"id\", monotonically_increasing_id()) \\\n",
        "    .select(\"id\", \"bulan\", \"tahun\")\n",
        "\n",
        "route_dim = fact_table.select(\"kode_trayek\", \"trayek\", \"jenis\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .withColumn(\"id\", monotonically_increasing_id()) \\\n",
        "    .select(\"id\", \"kode_trayek\", \"trayek\", \"jenis\")\n",
        "\n",
        "vehicle_dim = route_dim.select(\"jenis\") \\\n",
        "    .dropDuplicates() \\\n",
        "    .withColumn(\"id\", monotonically_increasing_id()) \\\n",
        "    .select(\"id\", \"jenis\")\n",
        "\n",
        "\n",
        "# Foreign keys dict\n",
        "vehicle_dict = vehicle_dim.select(\"jenis\", \"id\").rdd.collectAsMap()\n",
        "route_dict = route_dim.select(\"kode_trayek\", \"id\").rdd.collectAsMap()\n",
        "date_dict = date_dim.withColumn(\n",
        "    \"bulan_tahun\",\n",
        "    concat_ws(\n",
        "        \"_\",\n",
        "        col(\"bulan\"),\n",
        "        col(\"tahun\")\n",
        "    )\n",
        ").select(\"bulan_tahun\", \"id\").rdd.collectAsMap()\n",
        "\n",
        "# Creating foregin keys\n",
        "mapping_expr = create_map([lit(x) for x in chain(*vehicle_dict.items())])\n",
        "route_dim = route_dim.withColumn(\"vehicle_fk\", mapping_expr.__getitem__(col(\"jenis\")))\n",
        "\n",
        "mapping_expr = create_map([lit(x) for x in chain(*route_dict.items())])\n",
        "fact_table = fact_table.withColumn(\"route_fk\", mapping_expr.__getitem__(col(\"kode_trayek\")))\n",
        "\n",
        "mapping_expr = create_map([lit(x) for x in chain(*date_dict.items())])\n",
        "fact_table = fact_table.withColumn(\n",
        "    \"bulan_tahun\",\n",
        "    concat_ws(\n",
        "        \"_\",\n",
        "        col(\"bulan\"),\n",
        "        col(\"tahun\")\n",
        "    )\n",
        ").withColumn(\"date_fk\", mapping_expr.__getitem__(col(\"bulan_tahun\")))\n",
        "\n",
        "\n",
        "# Drop columns\n",
        "cols = [\"tahun\", \"bulan\", \"jenis\", \"kode_trayek\", \"trayek\", \"bulan_tahun\"]\n",
        "fact_table = fact_table.drop(*cols)\n",
        "route_dim = route_dim.drop(\"jenis\")\n",
        "\n",
        "\n",
        "# Define the BigQuery dataset and table name\n",
        "PROJECT_ID = \"golden-union-392713\"\n",
        "DATASET_NAME = \"transjakarta\"\n",
        "tables_list = [\n",
        "    (fact_table,'fact table'),\n",
        "    (route_dim, 'route dim'),\n",
        "    (date_dim, 'date dim'),\n",
        "    (vehicle_dim, 'vehicle dim'),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Write the DataFrame to BigQuery\n",
        "try:\n",
        "\n",
        "    for df, TABLE_NAME in tables_list:\n",
        "        df.write \\\n",
        "            .format(\"bigquery\") \\\n",
        "            .option(\"table\", f\"{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}\") \\\n",
        "            .option(\"temporaryGcsBucket\", \"transjakarta-data\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save()\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "nlcFJlv1vhAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zd3UwIjlk9AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZ_6BpromA_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark RDD"
      ],
      "metadata": {
        "id": "hHTBQFZaDrh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## French Bakery"
      ],
      "metadata": {
        "id": "opjLWmUFGypR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/french_bakery.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l_C_Cf9G1yg",
        "outputId": "4bfde942-feca-40ca-dd40-4a0c1f2d433d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/french_bakery.zip\n",
            "  inflating: Bakery sales.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import re\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "sZykYNMwG-3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1Vb5QfMI78F6",
        "outputId": "bb19ff67-6013-465c-fcb7-a7491a2f12bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0        date   time  ticket_number               article  \\\n",
              "0           0  2021-01-02  08:38       150040.0              BAGUETTE   \n",
              "1           1  2021-01-02  08:38       150040.0      PAIN AU CHOCOLAT   \n",
              "2           4  2021-01-02  09:14       150041.0      PAIN AU CHOCOLAT   \n",
              "3           5  2021-01-02  09:14       150041.0                  PAIN   \n",
              "4           8  2021-01-02  09:25       150042.0  TRADITIONAL BAGUETTE   \n",
              "\n",
              "   Quantity unit_price  \n",
              "0       1.0     0,90 €  \n",
              "1       3.0     1,20 €  \n",
              "2       2.0     1,20 €  \n",
              "3       1.0     1,15 €  \n",
              "4       5.0     1,20 €  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-920ac3cf-2cd4-4e56-b635-964c83713eb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>ticket_number</th>\n",
              "      <th>article</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>unit_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-02</td>\n",
              "      <td>08:38</td>\n",
              "      <td>150040.0</td>\n",
              "      <td>BAGUETTE</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0,90 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-02</td>\n",
              "      <td>08:38</td>\n",
              "      <td>150040.0</td>\n",
              "      <td>PAIN AU CHOCOLAT</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1,20 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2021-01-02</td>\n",
              "      <td>09:14</td>\n",
              "      <td>150041.0</td>\n",
              "      <td>PAIN AU CHOCOLAT</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1,20 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>2021-01-02</td>\n",
              "      <td>09:14</td>\n",
              "      <td>150041.0</td>\n",
              "      <td>PAIN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1,15 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>2021-01-02</td>\n",
              "      <td>09:25</td>\n",
              "      <td>150042.0</td>\n",
              "      <td>TRADITIONAL BAGUETTE</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1,20 €</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-920ac3cf-2cd4-4e56-b635-964c83713eb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-920ac3cf-2cd4-4e56-b635-964c83713eb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-920ac3cf-2cd4-4e56-b635-964c83713eb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `ticket number`: identifier for every single transaction\n",
        "- `article`: name of the product sold (in French)\n",
        "- `quantity`: quantity sold\n",
        "- `unit_price`: price per product\n",
        "- `Objective`: Forecast the sales in order to ease the production planning"
      ],
      "metadata": {
        "id": "yMFkVx6GKZGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the rdd\n",
        "lines = sc.textFile(\"Bakery sales.csv\")\n",
        "rdd = lines.map(lambda x: x.split(','))\n",
        "\n",
        "# Remove header\n",
        "header = rdd.first()\n",
        "rdd = rdd.filter(lambda x: x!= header)\n",
        "\n",
        "rdd.take(1)"
      ],
      "metadata": {
        "id": "anGbSHEcHmPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793d2119-604f-4d3b-83f1-b26a9372f601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['0', '2021-01-02', '08:38', '150040.0', 'BAGUETTE', '1.0', '\"0', '90 €\"']]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some rows contain more element\n",
        "rdd.map(lambda x: (len(x), 1)).reduceByKey(lambda x, y: x+y).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRkqd-c7-zg_",
        "outputId": "35669295-ad29-4d30-93fa-b68c537eab1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(8, 233984), (9, 21)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PLATPREPARE6,50 & PLATPREPARE7,00\n",
        "rdd.filter(lambda x: (len(x)==9)).collect()"
      ],
      "metadata": {
        "id": "svSwrcKq_Kh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ed2d69-8bbd-497b-ae11-6f6811653e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['137071',\n",
              "  '2021-07-12',\n",
              "  '08:25',\n",
              "  '187148.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"0',\n",
              "  '00 €\"'],\n",
              " ['137604',\n",
              "  '2021-07-12',\n",
              "  '11:30',\n",
              "  '187296.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '50\"',\n",
              "  '2.0',\n",
              "  '\"6',\n",
              "  '50 €\"'],\n",
              " ['137605',\n",
              "  '2021-07-12',\n",
              "  '11:30',\n",
              "  '187296.0',\n",
              "  '\"PLATPREPARE5',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"5',\n",
              "  '50 €\"'],\n",
              " ['137698',\n",
              "  '2021-07-12',\n",
              "  '11:52',\n",
              "  '187322.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '3.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['138743',\n",
              "  '2021-07-13',\n",
              "  '10:35',\n",
              "  '187606.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '4.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['138901',\n",
              "  '2021-07-13',\n",
              "  '11:17',\n",
              "  '187650.0',\n",
              "  '\"PLATPREPARE5',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"5',\n",
              "  '50 €\"'],\n",
              " ['138903',\n",
              "  '2021-07-13',\n",
              "  '11:17',\n",
              "  '187650.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"6',\n",
              "  '00 €\"'],\n",
              " ['139134',\n",
              "  '2021-07-13',\n",
              "  '12:19',\n",
              "  '187715.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['139559',\n",
              "  '2021-07-13',\n",
              "  '19:23',\n",
              "  '187833.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['139560',\n",
              "  '2021-07-13',\n",
              "  '19:23',\n",
              "  '187833.0',\n",
              "  '\"PLATPREPARE5',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"5',\n",
              "  '50 €\"'],\n",
              " ['254324',\n",
              "  '2021-11-04',\n",
              "  '09:01',\n",
              "  '218738.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"6',\n",
              "  '50 €\"'],\n",
              " ['254809',\n",
              "  '2021-11-04',\n",
              "  '13:15',\n",
              "  '218874.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['254883',\n",
              "  '2021-11-04',\n",
              "  '13:50',\n",
              "  '218896.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['254946',\n",
              "  '2021-11-04',\n",
              "  '17:18',\n",
              "  '218916.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '2.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['255090',\n",
              "  '2021-11-05',\n",
              "  '08:56',\n",
              "  '218957.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"6',\n",
              "  '50 €\"'],\n",
              " ['255528',\n",
              "  '2021-11-05',\n",
              "  '12:49',\n",
              "  '219080.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '50\"',\n",
              "  '1.0',\n",
              "  '\"6',\n",
              "  '50 €\"'],\n",
              " ['255608',\n",
              "  '2021-11-05',\n",
              "  '13:21',\n",
              "  '219101.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['255634',\n",
              "  '2021-11-05',\n",
              "  '13:28',\n",
              "  '219108.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['255757',\n",
              "  '2021-11-05',\n",
              "  '17:42',\n",
              "  '219141.0',\n",
              "  '\"PLATPREPARE6',\n",
              "  '50\"',\n",
              "  '2.0',\n",
              "  '\"6',\n",
              "  '50 €\"'],\n",
              " ['255766',\n",
              "  '2021-11-05',\n",
              "  '17:55',\n",
              "  '219144.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"'],\n",
              " ['255900',\n",
              "  '2021-11-06',\n",
              "  '09:35',\n",
              "  '219179.0',\n",
              "  '\"PLATPREPARE7',\n",
              "  '00\"',\n",
              "  '1.0',\n",
              "  '\"7',\n",
              "  '00 €\"']]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# columns transfrom\n",
        "def columns_transfrom(row):\n",
        "    id = row[3][:-2]\n",
        "    date = row[1]\n",
        "    time = row[2]\n",
        "    product = row[4]\n",
        "\n",
        "    quantity =  row[-3]\n",
        "    unit_price = f\"{row[-2]}.{row[-1]}\"\n",
        "\n",
        "    if len(row) == 9:\n",
        "        product = f\"{row[4]},{row[5]}\"\n",
        "\n",
        "    return (id, date, time, product, quantity, unit_price)\n",
        "\n",
        "rdd = rdd.map(columns_transfrom)\n",
        "rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfCo9hEeM7lq",
        "outputId": "dd2cefa2-44e6-4a6f-c204-c05bc669f2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('150040', '2021-01-02', '08:38', 'BAGUETTE', '1.0', '\"0.90 €\"'),\n",
              " ('150040', '2021-01-02', '08:38', 'PAIN AU CHOCOLAT', '3.0', '\"1.20 €\"'),\n",
              " ('150041', '2021-01-02', '09:14', 'PAIN AU CHOCOLAT', '2.0', '\"1.20 €\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quantity column\n",
        "def columns_transfrom_2(row):\n",
        "\n",
        "    id, date, time, product, quantity, unit_price = row\n",
        "\n",
        "    year, month, date = date.split('-')\n",
        "    year, month, date = int(year), int(month), int(date)\n",
        "\n",
        "    hour, minute = time.split(':')\n",
        "    hour, minute = int(hour), int(minute)\n",
        "\n",
        "    quantity = re.search('\\d(\\.|\")', quantity).group()[:-1]\n",
        "    quantity = int(quantity)\n",
        "\n",
        "    unit_price = re.search(r\"(\\d| )\\.\\d+\", unit_price).group()\n",
        "    unit_price = float(unit_price)\n",
        "\n",
        "    return (id, year, month, date, hour, minute, product, quantity, unit_price)\n",
        "\n",
        "rdd = rdd.map(columns_transfrom_2)\n",
        "rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZfMirXkFWxQ",
        "outputId": "d058eecb-7cc3-474f-a51e-2268fa2c2bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('150040', 2021, 1, 2, 8, 38, 'BAGUETTE', 1, 0.9),\n",
              " ('150040', 2021, 1, 2, 8, 38, 'PAIN AU CHOCOLAT', 3, 1.2),\n",
              " ('150041', 2021, 1, 2, 9, 14, 'PAIN AU CHOCOLAT', 2, 1.2)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Produk yang paling banyak terjual\n",
        "sorted(\n",
        "    rdd.map(lambda x: (x[-3], x[-2]))\\\n",
        "    .reduceByKey(lambda x, y: x+y).collect(),\n",
        "\n",
        "    key=lambda x: x[1], reverse=True\n",
        ")"
      ],
      "metadata": {
        "id": "X4Nzz2YVJjNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a872a4-1b2e-4928-ac0a-fb45ac2b58b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('TRADITIONAL BAGUETTE', 116873),\n",
              " ('CROISSANT', 28500),\n",
              " ('PAIN AU CHOCOLAT', 24548),\n",
              " ('COUPE', 23487),\n",
              " ('BANETTE', 22898),\n",
              " ('BAGUETTE', 21739),\n",
              " ('CEREAL BAGUETTE', 7509),\n",
              " ('SPECIAL BREAD', 5520),\n",
              " ('FORMULE SANDWICH', 5379),\n",
              " ('TARTELETTE', 5032),\n",
              " ('BOULE 400G', 4778),\n",
              " ('CAMPAGNE', 4326),\n",
              " ('COOKIE', 3763),\n",
              " ('ECLAIR', 3654),\n",
              " ('VIK BREAD', 3633),\n",
              " ('COMPLET', 3563),\n",
              " ('FICELLE', 3449),\n",
              " ('MOISSON', 3394),\n",
              " ('BANETTINE', 3120),\n",
              " ('BOULE 200G', 3098),\n",
              " ('PAIN BANETTE', 3043),\n",
              " ('SANDWICH COMPLET', 2914),\n",
              " ('PAIN AUX RAISINS', 2753),\n",
              " ('PAIN', 2409),\n",
              " ('CROISSANT AMANDES', 2369),\n",
              " ('KOUIGN AMANN', 2367),\n",
              " ('QUIM BREAD', 2153),\n",
              " ('CAFE OU EAU', 1966),\n",
              " ('CHAUSSON AUX POMMES', 1946),\n",
              " ('PAIN CHOCO AMANDES', 1919),\n",
              " ('BOISSON 33CL', 1904),\n",
              " ('BAGUETTE GRAINE', 1880),\n",
              " ('SAND JB EMMENTAL', 1832),\n",
              " ('BRIOCHE', 1720),\n",
              " ('GRAND FAR BRETON', 1506),\n",
              " ('TRAITEUR', 1476),\n",
              " ('SEIGLE', 1418),\n",
              " ('PARIS BREST', 1294),\n",
              " ('FINANCIER X5', 1275),\n",
              " ('DEMI BAGUETTE', 1147),\n",
              " ('FLAN', 1097),\n",
              " ('MILLES FEUILLES', 1046),\n",
              " ('SUCETTE', 1041),\n",
              " ('GD KOUIGN AMANN', 1027),\n",
              " ('DIVERS VIENNOISERIE', 850),\n",
              " ('TARTELETTE FRAISE', 819),\n",
              " ('FLAN ABRICOT', 801),\n",
              " ('PT NANTAIS', 751),\n",
              " ('NANTAIS', 599),\n",
              " ('FRAISIER', 589),\n",
              " ('DIVERS PATISSERIE', 538),\n",
              " ('BOULE POLKA', 525),\n",
              " ('SACHET VIENNOISERIE', 515),\n",
              " ('TROPEZIENNE', 508),\n",
              " ('DIVERS CONFISERIE', 503),\n",
              " ('SPECIAL BREAD KG', 467),\n",
              " ('SAVARIN', 467),\n",
              " ('NOIX JAPONAISE', 447),\n",
              " ('MACARON', 433),\n",
              " ('TARTE FRUITS 4P', 388),\n",
              " ('PLAT 7.60E', 338),\n",
              " ('GRANDE SUCETTE', 334),\n",
              " ('DIVERS BOULANGERIE', 327),\n",
              " ('FONDANT CHOCOLAT', 318),\n",
              " ('BOTTEREAU', 309),\n",
              " ('ROYAL', 293),\n",
              " ('TARTE FRUITS 6P', 283),\n",
              " ('PALET BRETON', 277),\n",
              " ('CHOU CHANTILLY', 271),\n",
              " ('DIVERS SANDWICHS', 241),\n",
              " ('VIENNOISE', 218),\n",
              " ('GAL FRANGIPANE 4P', 215),\n",
              " ('GD FAR BRETON', 177),\n",
              " ('TARTE FRAISE 4PER', 169),\n",
              " ('DEMI PAIN', 163),\n",
              " ('PAIN S/SEL', 152),\n",
              " ('PLAT 8.30E', 148),\n",
              " ('PAILLE', 135),\n",
              " ('GACHE', 131),\n",
              " ('SABLE F  P', 130),\n",
              " ('GAL FRANGIPANE 6P', 128),\n",
              " ('TARTE FRAISE 6P', 128),\n",
              " ('SACHET DE CROUTON', 125),\n",
              " ('RELIGIEUSE', 119),\n",
              " ('PLAT 7.00', 114),\n",
              " ('ST HONORE', 110),\n",
              " ('CARAMEL NOIX', 100),\n",
              " ('GAL POMME 4P', 99),\n",
              " ('GD NANTAIS', 99),\n",
              " ('FRAMBOISIER', 97),\n",
              " ('DIVERS BOISSONS', 88),\n",
              " ('TRIANGLES', 83),\n",
              " ('CHOCOLAT', 82),\n",
              " ('ROYAL 4P', 78),\n",
              " ('BAGUETTE APERO', 76),\n",
              " ('12 MACARON', 70),\n",
              " ('ROYAL 6P', 68),\n",
              " ('TARTE FINE', 67),\n",
              " ('BRIOCHETTE', 66),\n",
              " ('MERINGUE', 65),\n",
              " ('GAL POMME 6P', 62),\n",
              " ('TARTELETTE CHOC', 62),\n",
              " ('DELICETROPICAL', 61),\n",
              " ('BROWNIES', 47),\n",
              " ('PAIN DE MIE', 40),\n",
              " ('ENTREMETS', 38),\n",
              " ('NID DE POULE', 35),\n",
              " ('TROPEZIENNE FRAMBOISE', 33),\n",
              " ('GAL POIRE CHOCO 4P', 24),\n",
              " ('ECLAIR FRAISE PISTACHE', 22),\n",
              " ('FINANCIER', 22),\n",
              " ('PAIN SUISSE PEPITO', 21),\n",
              " ('BRIOCHE DE NOEL', 19),\n",
              " ('\"PLATPREPARE7,00\"', 17),\n",
              " ('GAL POIRE CHOCO 6P', 15),\n",
              " ('FORMULE PLAT PREPARE', 11),\n",
              " ('BUCHE 4PERS', 11),\n",
              " ('PLAT', 9),\n",
              " ('CRUMBLE', 9),\n",
              " ('REDUCTION SUCREES 12', 8),\n",
              " ('\"PLATPREPARE6,50\"', 8),\n",
              " ('BUCHE 6PERS', 8),\n",
              " ('.', 7),\n",
              " ('THE', 7),\n",
              " ('PALMIER', 6),\n",
              " ('SAND JB', 5),\n",
              " ('PT PLATEAU SALE', 4),\n",
              " ('TULIPE', 4),\n",
              " ('ARMORICAIN', 3),\n",
              " ('PAIN GRAINES', 3),\n",
              " ('\"PLATPREPARE5,50\"', 3),\n",
              " ('CRUMBLECARAMEL OU PISTAE', 3),\n",
              " ('GUERANDAIS', 3),\n",
              " ('PATES', 2),\n",
              " ('GALETTE 8 PERS', 2),\n",
              " ('GD PLATEAU SALE', 2),\n",
              " ('PLAT 6.50E', 2),\n",
              " ('FORMULE PATE', 2),\n",
              " ('BUCHE 8PERS', 2),\n",
              " ('PLAQUE TARTE 25P', 2),\n",
              " ('TARTELETTE COCKTAIL', 2),\n",
              " ('CAKE', 1),\n",
              " ('\"PLATPREPARE6,00\"', 1),\n",
              " ('ARTICLE 295', 1),\n",
              " ('REDUCTION SUCREES 24', 1),\n",
              " ('DOUCEUR D HIVER', 1),\n",
              " ('TROIS CHOCOLAT', 1),\n",
              " ('PAIN NOIR', 1),\n",
              " ('SACHET DE VIENNOISERIE', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transaksi dengan pendapatan paling besar\n",
        "result = rdd.map(lambda x: (x[0], x[-1]*x[-2]))\\\n",
        "            .reduceByKey(lambda x, y: x+y)\\\n",
        "            .collect()\n",
        "\n",
        "\n",
        "result = sorted(result, key=lambda x: x[1], reverse=True)[:10]\n",
        "print('Transaction Id \\t Income')\n",
        "for item in result:\n",
        "    print(f\"{item[0]} \\t\\t {item[1]} €\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQyRJj84E9Ma",
        "outputId": "a89c088b-8524-4904-b4ec-21919305830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transaction Id \t Income\n",
            "194199 \t\t 79.7 €\n",
            "256455 \t\t 64.2 €\n",
            "220468 \t\t 61.2 €\n",
            "197217 \t\t 58.6 €\n",
            "198481 \t\t 57.2 €\n",
            "198573 \t\t 54.2 €\n",
            "273733 \t\t 52.800000000000004 €\n",
            "272070 \t\t 49.9 €\n",
            "229276 \t\t 49.0 €\n",
            "270964 \t\t 48.9 €\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pendapatan per tahun\n",
        "result = rdd.map(lambda x: (x[1], x[-1]*x[-2]))\\\n",
        "            .reduceByKey(lambda x, y: x+y)\\\n",
        "            .collect()\n",
        "\n",
        "print('Year \\t Income')\n",
        "for item in result:\n",
        "    print(f\"{item[0]} \\t {item[1]:.2f} €\")"
      ],
      "metadata": {
        "id": "dwcZP9DQFE50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff44f73-7efe-4848-b983-542defef05f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Year \t Income\n",
            "2022 \t 246857.83 €\n",
            "2021 \t 294123.72 €\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pendapatan per bulan\n",
        "result = rdd.map(lambda x: (x[2], x[-1]*x[-2]))\\\n",
        "            .reduceByKey(lambda x, y: x+y)\\\n",
        "            .mapValues(lambda x: x/2)\\\n",
        "            .collect()\n",
        "\n",
        "print('Month \\t Avg Income')\n",
        "for item in sorted(result, key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{item[0]} \\t {item[1]:.2f} €\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrIrLBN5IkQa",
        "outputId": "22ea5eeb-5adb-4e9a-9555-38d6186a259d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Month \t Avg Income\n",
            "8 \t 49552.65 €\n",
            "7 \t 42699.34 €\n",
            "5 \t 28485.38 €\n",
            "4 \t 24284.75 €\n",
            "6 \t 24260.28 €\n",
            "9 \t 21611.80 €\n",
            "3 \t 19706.01 €\n",
            "2 \t 16978.22 €\n",
            "1 \t 14352.58 €\n",
            "10 \t 10938.83 €\n",
            "12 \t 8875.32 €\n",
            "11 \t 8745.62 €\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pada jam transaksi cenderung terjadi\n",
        "# Pagi      3.00 - 9.59\n",
        "# Siang     10.00 - 17.59\n",
        "# Malam     18.00 - 2.59\n",
        "\n",
        "def when_to_buy(row):\n",
        "    when = 'night'\n",
        "    if (row[4] >= 3) and (row[4] <= 9):\n",
        "        when = 'morning'\n",
        "    elif (row[4] >= 10) and (row[4] <= 17):\n",
        "        when = 'noon'\n",
        "\n",
        "    return (row[0], when)\n",
        "\n",
        "result = rdd.map(when_to_buy)\\\n",
        "            .reduceByKey(lambda x, y: x)\\\n",
        "            .map(lambda x: (x[1], 1))\\\n",
        "            .reduceByKey(lambda x, y: x+y)\\\n",
        "            .collect()\n",
        "\n",
        "for item in result:\n",
        "    print(f\"{item[0]}    \\t{item[1]} transcations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6QeBzcKCWRy",
        "outputId": "786bbac5-90cc-4667-e59a-cac7883cb5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noon    \t93182 transcations\n",
            "night    \t7090 transcations\n",
            "morning    \t36179 transcations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S4CiHclOIhT",
        "outputId": "1b9f868e-edec-435d-a211-6c287488da30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('150040', 2021, 1, 2, 8, 38, 'BAGUETTE', 1, 0.9),\n",
              " ('150040', 2021, 1, 2, 8, 38, 'PAIN AU CHOCOLAT', 3, 1.2)]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INDO INGGRIS WOYYYYYYYYYYYYYYYYYYYYY"
      ],
      "metadata": {
        "id": "urlpAtyvL6Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instagram App Store Reviews"
      ],
      "metadata": {
        "id": "0cjCtNwo8XLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import re\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "3l935BkY8bkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the rdd\n",
        "# source: https://www.kaggle.com/datasets/odins0n/top-20-play-store-app-reviews-daily-update?select=all_combined.csv\n",
        "rdd = sc.textFile(\"Instagram.csv\")\n",
        "\n",
        "rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxY-PeyQ8eFB",
        "outputId": "8fa6602d-c698-4cf2-8243-ba8305598eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['reviewId,content,score',\n",
              " '83c7935d-d791-4d9c-b738-c7984992491a,It is a best app,5',\n",
              " '2d56d1a3-a4d0-4482-ae45-8394d2b8951f,\"I love Insta, my No1 social media app❤️\",5']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove header\n",
        "header = rdd.first()\n",
        "rdd = rdd.filter(lambda x: x!= header)\n",
        "\n",
        "# Get the information about rating and review\n",
        "def parser(line):\n",
        "    row = line.split(',')\n",
        "    rating, review = row[-1], row[1]\n",
        "    for i in range(2, len(row)-1):\n",
        "        review += row[i]\n",
        "\n",
        "    return (rating, review)\n",
        "\n",
        "rdd = rdd.map(parser)\n",
        "\n",
        "rdd.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0_mNXrzALX0",
        "outputId": "e332f48d-2898-41bc-e891-b4b7c3858ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('5', 'It is a best app'), ('5', '\"I love Insta my No1 social media app❤️\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing ratings\n",
        "missing_ratings = rdd.filter(lambda x: re.search('^\\D', x[0])).collect()\n",
        "rdd = rdd.filter(lambda x: x not in missing_ratings)\n",
        "\n",
        "missing_ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39E03w4IDJDQ",
        "outputId": "45d12c47-5724-4fc6-87f7-0e2affa5b127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\"Cool app... Watch only what u follow... Free from politics and ads... ',\n",
              "  '\"Cool app... Watch only what u follow... Free from politics and ads... ')]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dist of ratings\n",
        "result = rdd.mapValues(lambda x: 1)\\\n",
        "            .reduceByKey(lambda x, y: x+y)\\\n",
        "            .collect()\n",
        "\n",
        "for item in result:\n",
        "    print(f\"{int(item[0])}   {item[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmQKWYz5-0Im",
        "outputId": "cae1744a-99ec-41fd-9be4-8a03f6252391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1   1604\n",
            "4   836\n",
            "5   6643\n",
            "3   526\n",
            "2   391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Good ratings RDD\n",
        "goodRDD = rdd.filter(lambda x: x[0] in ['4', '5'])\n",
        "goodRDD.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvH1MF3NGrxq",
        "outputId": "5e3d987c-a8dd-43d8-ff3d-c1f95b358311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('5', 'It is a best app'),\n",
              " ('5', '\"I love Insta my No1 social media app❤️\"'),\n",
              " ('5', '\"THIS IS A VERY GOOD APP 👌🙂\"'),\n",
              " ('5', 'Nice app'),\n",
              " ('5', 'Good reel and good dance')]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.filter(lambda x: '5' in x[1]).take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FL86w8iJnKj",
        "outputId": "2dea04ff-b6ab-4ca9-e012-b8173354ced1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('5', '\"No much talk I suppose give 7 🌟 but it\\'s end in 5\"'),\n",
              " ('5', 'Janychowbhury05'),\n",
              " ('2',\n",
              "  '\"I\\'ve noticed an issue with checking my notifications. I receive notifications just fine but when I enter the app I cannot see what I got especially messages. You\\'d think it\\'s an internet issue but it happens with or without wifi even with data even with 5G. It\\'s bad enough getting spam accounts and bots harassing you in the comments. There are other little bugs here and there too such as when you save a post and it says \"\"saved\"\" but it stays on for a while or even appears stuck.\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatmap to normalize words\n",
        "goodRDD = goodRDD.flatMap(lambda x: re.compile(r'\\W+').split(x[1].lower()))\n",
        "\n",
        "# Results\n",
        "sorted(\n",
        "    goodRDD.map(lambda x: (x, 1))\\\n",
        "        .reduceByKey(lambda x, y: x + y)\\\n",
        "        .collect(),\n",
        "\n",
        "    key=lambda x: x[1], reverse=True\n",
        ")[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fITpDwoFIqwY",
        "outputId": "43831a8e-0989-4fa1-edff-c6e033fb6341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 2624),\n",
              " ('app', 1710),\n",
              " ('good', 1342),\n",
              " ('i', 1080),\n",
              " ('nice', 1068),\n",
              " ('very', 897),\n",
              " ('it', 768),\n",
              " ('is', 669),\n",
              " ('and', 623),\n",
              " ('instagram', 616),\n",
              " ('this', 613),\n",
              " ('to', 552),\n",
              " ('the', 535),\n",
              " ('best', 505),\n",
              " ('my', 471),\n",
              " ('love', 463),\n",
              " ('a', 388),\n",
              " ('s', 365),\n",
              " ('you', 318),\n",
              " ('for', 313),\n",
              " ('but', 270),\n",
              " ('so', 262),\n",
              " ('super', 256),\n",
              " ('like', 232),\n",
              " ('please', 215),\n",
              " ('of', 206),\n",
              " ('great', 203),\n",
              " ('amazing', 198),\n",
              " ('not', 197),\n",
              " ('me', 188),\n",
              " ('in', 184),\n",
              " ('aap', 158),\n",
              " ('t', 153),\n",
              " ('awesome', 146),\n",
              " ('can', 141),\n",
              " ('excellent', 137),\n",
              " ('have', 134),\n",
              " ('that', 134),\n",
              " ('with', 130),\n",
              " ('use', 119),\n",
              " ('on', 106),\n",
              " ('social', 103),\n",
              " ('hai', 102),\n",
              " ('insta', 99),\n",
              " ('your', 98),\n",
              " ('application', 95),\n",
              " ('are', 92),\n",
              " ('all', 92),\n",
              " ('really', 90),\n",
              " ('time', 89)]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bad ratings RDD\n",
        "badRDD = rdd.filter(lambda x: x[0] in ['1', '2'])\n",
        "badRDD.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC5IP9BHH-iC",
        "outputId": "c0046f9d-5483-4c9b-b647-9a70149bca01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1',\n",
              "  \"I would have given you 0 stars if i could. Fix your algorithm I'm being spammed by sex workers acounts with sketchy Links that Instagram don't see a problem in when I report it. I'm starting to hate this app and it's stupid algorithm. Just make the mobile version like the desktop version and stop this shitfest!\"),\n",
              " ('1', 'Worst I want to uninstall this'),\n",
              " ('1', 'Reply problem please solved this problem'),\n",
              " ('1', \"It's hanging\"),\n",
              " ('1',\n",
              "  '\"Internet connectivity gets bugged way too often  re-downloading fixes it but not for long and you\\'ll have to do the same again over and over. Ok so now re-downloading also doesn\\'t help very nice 🙂\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatmap to normalize words\n",
        "badRDD = badRDD.flatMap(lambda x: re.compile(r'\\W+').split(x[1].lower()))\n",
        "\n",
        "# Results\n",
        "sorted(\n",
        "    badRDD.map(lambda x: (x, 1))\\\n",
        "        .reduceByKey(lambda x, y: x + y)\\\n",
        "        .collect(),\n",
        "\n",
        "    key=lambda x: x[1], reverse=True\n",
        ")[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxVVuCaJ9Ube",
        "outputId": "9c67a95b-7c86-465e-b9ad-f2fdab8c2a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 1045),\n",
              " ('i', 963),\n",
              " ('to', 634),\n",
              " ('the', 590),\n",
              " ('it', 564),\n",
              " ('my', 553),\n",
              " ('and', 544),\n",
              " ('not', 483),\n",
              " ('app', 460),\n",
              " ('is', 455),\n",
              " ('t', 423),\n",
              " ('instagram', 354),\n",
              " ('this', 327),\n",
              " ('a', 315),\n",
              " ('in', 264),\n",
              " ('of', 245),\n",
              " ('account', 220),\n",
              " ('can', 213),\n",
              " ('but', 210),\n",
              " ('for', 207),\n",
              " ('s', 195),\n",
              " ('on', 194),\n",
              " ('you', 191),\n",
              " ('please', 182),\n",
              " ('me', 179),\n",
              " ('are', 152),\n",
              " ('very', 150),\n",
              " ('that', 148),\n",
              " ('have', 143),\n",
              " ('good', 140),\n",
              " ('so', 137),\n",
              " ('problem', 130),\n",
              " ('don', 129),\n",
              " ('no', 124),\n",
              " ('many', 120),\n",
              " ('as', 117),\n",
              " ('with', 113),\n",
              " ('all', 109),\n",
              " ('update', 107),\n",
              " ('working', 106),\n",
              " ('when', 104),\n",
              " ('even', 101),\n",
              " ('bad', 101),\n",
              " ('option', 99),\n",
              " ('there', 95),\n",
              " ('am', 93),\n",
              " ('like', 89),\n",
              " ('post', 85),\n",
              " ('nice', 84),\n",
              " ('now', 83)]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Romance Book"
      ],
      "metadata": {
        "id": "dalT-S3h9-NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import re\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "6E9zxIda-ZTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.textFile(\"romance.txt\")\n",
        "\n",
        "# Flatmap to normalize words\n",
        "rdd = rdd.flatMap(lambda x: re.compile(r'\\W+').split(x.lower()))"
      ],
      "metadata": {
        "id": "w6qp3U7B9_BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(\n",
        "    rdd.map(lambda x: (x, 1))\\\n",
        "        .reduceByKey(lambda x, y: x + y)\\\n",
        "        .collect(),\n",
        "\n",
        "    key=lambda x: x[1], reverse=True\n",
        ")[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC4jtc20DA-J",
        "outputId": "0380545b-df63-4e9c-f251-61b31eca69ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 5086),\n",
              " ('the', 877),\n",
              " ('and', 806),\n",
              " ('i', 659),\n",
              " ('to', 626),\n",
              " ('a', 547),\n",
              " ('of', 518),\n",
              " ('in', 394),\n",
              " ('is', 372),\n",
              " ('that', 369),\n",
              " ('you', 368),\n",
              " ('my', 356),\n",
              " ('s', 324),\n",
              " ('romeo', 320),\n",
              " ('with', 300),\n",
              " ('not', 288),\n",
              " ('thou', 278),\n",
              " ('this', 266),\n",
              " ('me', 266),\n",
              " ('for', 251),\n",
              " ('it', 244),\n",
              " ('d', 242),\n",
              " ('be', 227),\n",
              " ('juliet', 193),\n",
              " ('but', 185),\n",
              " ('what', 174),\n",
              " ('o', 173),\n",
              " ('thy', 170),\n",
              " ('as', 168),\n",
              " ('capulet', 163)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark SQL"
      ],
      "metadata": {
        "id": "2QfTCuIVO8pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Go to College?"
      ],
      "metadata": {
        "id": "yYSEZNvJFHHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions\n",
        "spark = SparkSession.builder.appName('app').getOrCreate()"
      ],
      "metadata": {
        "id": "6G23prCpK-Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('go_to_college.csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "KWKOtSIBLb_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns' Explanation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> `type_school` : Type of school that student attends\\\n",
        "> `school_accreditation` : Quality of school. A is better than B.\\\n",
        "> `gender` : Gender of student\\\n",
        "> `interest` : How interested are students if they go to college\\\n",
        "> `residence` : Type of residence\\\n",
        "> `parent_age` : Parent age\\\n",
        "> `parent_salary` : Parent salary per month in IDR/Rupiah\\\n",
        "> `house_area` : Parent house area in meter square\\\n",
        "> `average_grades` : Average of grades in scale of 0-100\\\n",
        "> `parent_was_in_college` : Was parent ever in college?\\\n",
        "> `will_go_to_college` : Predictions about going to college\\"
      ],
      "metadata": {
        "id": "fggncblwR7Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ROkHc-NLrv-",
        "outputId": "2490dbf5-0075-4b32-b42e-4ecafa1fade3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- type_school: string (nullable = true)\n",
            " |-- school_accreditation: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- interest: string (nullable = true)\n",
            " |-- residence: string (nullable = true)\n",
            " |-- parent_age: integer (nullable = true)\n",
            " |-- parent_salary: integer (nullable = true)\n",
            " |-- house_area: double (nullable = true)\n",
            " |-- average_grades: double (nullable = true)\n",
            " |-- parent_was_in_college: boolean (nullable = true)\n",
            " |-- will_go_to_college: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFVOtg6rL-Yj",
        "outputId": "a064585a-d5a0-4488-a97e-79e4343c7567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('type_school', 'string'),\n",
              " ('school_accreditation', 'string'),\n",
              " ('gender', 'string'),\n",
              " ('interest', 'string'),\n",
              " ('residence', 'string'),\n",
              " ('parent_age', 'int'),\n",
              " ('parent_salary', 'int'),\n",
              " ('house_area', 'double'),\n",
              " ('average_grades', 'double'),\n",
              " ('parent_was_in_college', 'boolean'),\n",
              " ('will_go_to_college', 'boolean')]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['parent_age', 'parent_salary', 'house_area', 'average_grades']\n",
        "# df.select(cols).describe().show()\n",
        "df.select(cols).summary().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OzP30JjL_zx",
        "outputId": "aa8fdf12-9a76-4fc8-d043-ffde24ad2f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+------------------+------------------+------------------+\n",
            "|summary|       parent_age|     parent_salary|        house_area|    average_grades|\n",
            "+-------+-----------------+------------------+------------------+------------------+\n",
            "|  count|             1000|              1000|              1000|              1000|\n",
            "|   mean|           52.208|         5381570.0| 74.51530000000005| 86.09719999999999|\n",
            "| stddev|3.500426972383368|1397545.9096822797|15.293345687989016|3.3787384085236942|\n",
            "|    min|               40|           1000000|              20.0|              75.0|\n",
            "|    25%|               50|           4360000|              64.6|             83.73|\n",
            "|    50%|               52|           5440000|              75.5|             85.57|\n",
            "|    75%|               54|           6380000|              84.8|             88.26|\n",
            "|    max|               65|          10000000|             120.0|              98.0|\n",
            "+-------+-----------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical Columns"
      ],
      "metadata": {
        "id": "2DGLXpO3-KYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "> Many of `Very Interested` and `Interested` student are not going to college. The opposite also happened to the  `Not Interested` and `Less Interested` student. We can see that interest doesn't linearly correlated with college admission. Even so, the presence of significance can make us use `interest` column as a predictor.\n",
        "\n",
        "> Other categorical columns don't give siginificant difference in affecting student to go to college."
      ],
      "metadata": {
        "id": "avF2YpU0WOcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# interest\n",
        "col = 'interest'\n",
        "for value in df.select(col).distinct().collect():\n",
        "\n",
        "    print(value[0])\n",
        "    df.filter(df[col]==value[0])\\\n",
        "        .groupBy('will_go_to_college')\\\n",
        "        .count()\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOI4pxB-Pvj",
        "outputId": "7091c5a5-6403-4e75-f109-ef7011d5b585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Very Interested\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  190|\n",
            "|             false|  134|\n",
            "+------------------+-----+\n",
            "\n",
            "Less Interested\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   87|\n",
            "|             false|  142|\n",
            "+------------------+-----+\n",
            "\n",
            "Uncertain\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  158|\n",
            "|             false|  103|\n",
            "+------------------+-----+\n",
            "\n",
            "Interested\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   19|\n",
            "|             false|   81|\n",
            "+------------------+-----+\n",
            "\n",
            "Not Interested\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   46|\n",
            "|             false|   40|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# type_school\n",
        "col = 'type_school'\n",
        "for value in df.select(col).distinct().collect():\n",
        "\n",
        "    print(value[0])\n",
        "    df.filter(df[col]==value[0])\\\n",
        "        .groupBy('will_go_to_college')\\\n",
        "        .count()\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9vZdX0G_64",
        "outputId": "497ab931-4254-4c84-ac37-52072352f8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocational\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  187|\n",
            "|             false|  204|\n",
            "+------------------+-----+\n",
            "\n",
            "Academic\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  313|\n",
            "|             false|  296|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# school_accreditation\n",
        "col = 'school_accreditation'\n",
        "for value in df.select(col).distinct().collect():\n",
        "\n",
        "    print(value[0])\n",
        "    df.filter(df[col]==value[0])\\\n",
        "        .groupBy('will_go_to_college')\\\n",
        "        .count()\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD6TPWRTH5pI",
        "outputId": "128e4cd9-8e7a-43ea-f82b-f05cba88d00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  252|\n",
            "|             false|  267|\n",
            "+------------------+-----+\n",
            "\n",
            "A\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  248|\n",
            "|             false|  233|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gender\n",
        "col = 'gender'\n",
        "for value in df.select(col).distinct().collect():\n",
        "\n",
        "    print(value[0])\n",
        "    df.filter(df[col]==value[0])\\\n",
        "        .groupBy('will_go_to_college')\\\n",
        "        .count()\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGdBFwcXICKs",
        "outputId": "aab02b65-0beb-4121-b907-d0a7351db3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Female\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  251|\n",
            "|             false|  234|\n",
            "+------------------+-----+\n",
            "\n",
            "Male\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  249|\n",
            "|             false|  266|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# residence\n",
        "col = 'residence'\n",
        "for value in df.select(col).distinct().collect():\n",
        "\n",
        "    print(value[0])\n",
        "    df.filter(df[col]==value[0])\\\n",
        "        .groupBy('will_go_to_college')\\\n",
        "        .count()\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Y9_zMLINXg",
        "outputId": "6602e1d3-09f7-4e43-ccc5-36b663e31cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urban\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  271|\n",
            "|             false|  268|\n",
            "+------------------+-----+\n",
            "\n",
            "Rural\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  229|\n",
            "|             false|  232|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parent_was_in_college\n",
        "col = 'parent_was_in_college'\n",
        "for value in df.select(col).distinct().collect():\n",
        "\n",
        "    print(value[0])\n",
        "    df.filter(df[col]==value[0])\\\n",
        "        .groupBy('will_go_to_college')\\\n",
        "        .count()\\\n",
        "        .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bJkAnBnIQ3T",
        "outputId": "0637e155-4b48-452b-b585-31c75d1721d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  272|\n",
            "|             false|  248|\n",
            "+------------------+-----+\n",
            "\n",
            "False\n",
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  228|\n",
            "|             false|  252|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Columns"
      ],
      "metadata": {
        "id": "h99wBJKWGq8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "\n",
        "> 1.   Good grade students tend to go to college and bad grade students tend not to go to college.\n",
        "2.   Old and young parents tend to hold their children from going to college.\n",
        "3.   Parents' income and their home are positively correlated with sending their children to college.\n",
        "\n",
        ">*Warning* :\\\n",
        "We have not examined the possibility of multicollinearity.\n",
        "\n"
      ],
      "metadata": {
        "id": "yTTfgnttUyRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical columns + summary\n",
        "cols = ['summary', 'parent_age', 'parent_salary', 'house_area', 'average_grades']\n",
        "\n",
        "# Students who will go to college\n",
        "print('Students who will go to college')\n",
        "df.filter(df['will_go_to_college']=='true').summary().select(cols).show()\n",
        "\n",
        "# Students who wont go to college\n",
        "print('Students who wont go to college')\n",
        "df.filter(df['will_go_to_college']=='false').summary().select(cols).show()"
      ],
      "metadata": {
        "id": "tLPtCWfFMZpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9c305b-9294-4c6d-cd4c-bd28ca89e3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students who will go to college\n",
            "+-------+------------------+-----------------+-----------------+-----------------+\n",
            "|summary|        parent_age|    parent_salary|       house_area|   average_grades|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+\n",
            "|  count|               500|              500|              500|              500|\n",
            "|   mean|            52.358|        6046040.0|81.65779999999991|87.80306000000002|\n",
            "| stddev|2.9966307065072875|1212242.129428243|12.84237877283995|3.568375125806835|\n",
            "|    min|                41|          1000000|             40.9|             75.0|\n",
            "|    25%|                51|          5300000|             74.4|            85.41|\n",
            "|    50%|                52|          6060000|             81.3|            87.64|\n",
            "|    75%|                54|          6730000|             90.1|            90.28|\n",
            "|    max|                64|         10000000|            120.0|             98.0|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+\n",
            "\n",
            "Students who wont go to college\n",
            "+-------+-----------------+-----------------+------------------+-----------------+\n",
            "|summary|       parent_age|    parent_salary|        house_area|   average_grades|\n",
            "+-------+-----------------+-----------------+------------------+-----------------+\n",
            "|  count|              500|              500|               500|              500|\n",
            "|   mean|           52.058|        4717100.0| 67.37280000000003|84.39134000000004|\n",
            "| stddev|3.937721078820742|1247333.906276702|14.180260213389564|2.071169498274139|\n",
            "|    min|               40|          1660000|              20.0|            77.18|\n",
            "|    25%|               49|          3780000|              58.8|            83.08|\n",
            "|    50%|               52|          4650000|              67.2|            84.44|\n",
            "|    75%|               55|          5650000|              77.2|            85.63|\n",
            "|    max|               65|          8370000|             113.2|            90.97|\n",
            "+-------+-----------------+-----------------+------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Good grades\n",
        "filt = df['average_grades'] > 90\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5hBoroDNqii",
        "outputId": "7f014ae7-02d0-4d80-df17-6b33a8f7e5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  131|\n",
            "|             false|    2|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bad grades\n",
        "filt = df['average_grades'] < 83\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_TWaDuUOMYJ",
        "outputId": "78429b5e-f3a9-439b-ab3e-d8f98376d780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   39|\n",
            "|             false|  117|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Old parents\n",
        "filt = df['parent_age'] > 55\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjdo_7h3Oed3",
        "outputId": "3534ed95-98ca-4e95-c163-5f2a913d9027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   59|\n",
            "|             false|   92|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Young parents\n",
        "filt = df['parent_age'] < 48\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewPWzp3OnZQ",
        "outputId": "eec68835-b174-4cbc-9afd-12b60b367c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   25|\n",
            "|             false|   64|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Middle age parents\n",
        "filt = (df['parent_age'] > 50) & (df['parent_age'] < 54)\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7A22EbhO2cy",
        "outputId": "692e16ba-4437-4e47-b1ee-2fb42d0a99e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  210|\n",
            "|             false|  154|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quartile of parent_salary\n",
        "q1, q2, q3 = df.approxQuantile(\"parent_salary\", [0.25, 0.5, 0.75], 0)\n",
        "q1, q2, q3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzBm90n-RDkX",
        "outputId": "1566859c-a8c2-473a-8c18-6d07925a4b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4360000.0, 5440000.0, 6380000.0)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relative low income salary\n",
        "filt = df['parent_salary'] < q1\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W9heymLPrJz",
        "outputId": "e8a7d46d-b3d6-43a1-8942-1ca953bfe3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   36|\n",
            "|             false|  213|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relative high income salary\n",
        "filt = df['parent_salary'] > q3\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJmhZLj1R9X7",
        "outputId": "a670b21b-d3d6-41da-9ced-297effcf7230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  197|\n",
            "|             false|   53|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Relative middle income salary\n",
        "filt = (df['parent_salary'] > q1) & (df['parent_salary'] < q3)\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79PRoNWTR-lf",
        "outputId": "b6e38d5b-f4ad-4a34-c9e5-4907443f5961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  266|\n",
            "|             false|  232|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Median or higher salary\n",
        "filt = df['parent_salary'] > q2\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECXRGGMxSI3i",
        "outputId": "362bbee6-8d6c-4103-e1b7-a4ef2f011724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  353|\n",
            "|             false|  146|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quartile of house area\n",
        "q1, q2, q3 = df.approxQuantile(\"house_area\", [0.25, 0.5, 0.75], 0)\n",
        "q1, q2, q3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfMWwB20TtqW",
        "outputId": "01701ff8-8cab-499d-ea48-7c4a8574668a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64.6, 75.5, 84.8)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small house_area\n",
        "filt = df['house_area'] < q1\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1479db-c2fe-4c68-a631-33f35867c1ad",
        "id": "jenhEgnaUPnD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|   46|\n",
            "|             false|  200|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Large house area\n",
        "filt = df['house_area'] > q3\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j73FHQJ9UXUu",
        "outputId": "480b16f1-d402-4e76-e2b5-e54ad8375930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  198|\n",
            "|             false|   52|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Middle house area\n",
        "filt = (df['house_area'] > q1) & (df['house_area'] < q3)\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb00fd15-879f-4b38-fc30-eec57e456bcd",
        "id": "pfFXJcwRUPnD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  253|\n",
            "|             false|  243|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Median or larger house area\n",
        "filt = df['house_area'] > q2\n",
        "df.filter(filt).groupBy('will_go_to_college').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99a8811-2a8f-4400-d7c6-c1fa311d48f9",
        "id": "943eO_EdUPnD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|will_go_to_college|count|\n",
            "+------------------+-----+\n",
            "|              true|  350|\n",
            "|             false|  148|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KJ + Notes Udemy\n"
      ],
      "metadata": {
        "id": "N_8U4vtVWZog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2\n",
        "Spark Basics and the RDD Interface"
      ],
      "metadata": {
        "id": "HeyRnp4gWzV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`keys` itu identifier yang berada di paling kiri."
      ],
      "metadata": {
        "id": "r6EJWwkQbqaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ratings-counter.py\n"
      ],
      "metadata": {
        "id": "noBnnqc7Wdjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data yg akan diproses:\\\n",
        "(user id, movie id, rating value, timestamp)\n",
        "\n",
        "```\n",
        "196 242 3 881250949\n",
        "186 302 3 891717742\n",
        "22  377 1 878887116\n",
        "244  51 2 880606923\n",
        "166 346 1 886397596\n",
        " .   .  .     .\n",
        " .   .  .     .\n",
        " .   .  .     .\n",
        "```"
      ],
      "metadata": {
        "id": "XU5TZxVYTbFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "lines = sc.textFile(\"file:///SparkCourse/ml-100k/u.data\")\n",
        "ratings = lines.map(lambda x: x.split()[2])\n",
        "result = ratings.countByValue()\n",
        "\n",
        "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
        "for key, value in sortedResults.items():\n",
        "    print(\"%s %i\" % (key, value))"
      ],
      "metadata": {
        "id": "6i5Eq1eBWkPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# penjelasan `sorted(result.items())`\n",
        "result = dict(zip([3,1,2], [2,2,1]))\n",
        "\n",
        "print(result)\n",
        "sorted(result.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISutBGhDVTbJ",
        "outputId": "f25ab22a-0412-4cf2-c41e-482404b236d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{3: 2, 1: 2, 2: 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2), (2, 1), (3, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### friends-by-age.py"
      ],
      "metadata": {
        "id": "85V5_vg0XZbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(id, name, age, number_of_friends)"
      ],
      "metadata": {
        "id": "jz3XXZ10YSXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    age = int(fields[2])\n",
        "    numFriends = int(fields[3])\n",
        "    return (age, numFriends)\n",
        "\n",
        "lines = sc.textFile(\"file:///SparkCourse/fakefriends.csv\")\n",
        "rdd = lines.map(parseLine)\n",
        "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
        "results = averagesByAge.collect()\n",
        "for result in results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "YI6Dn1jaXZAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### min-temperatures.py"
      ],
      "metadata": {
        "id": "ruZWPFW8YiSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(weather station id, date, jens faktor iklim, nilai faktor iklim, data tidak berguna)\n",
        "\n",
        "```\n",
        "ITE00100554,18000101,TMAX,-75,,,E,\n",
        "ITE00100554,18000101,TMIN,-148,,,E,\n",
        "GM000010962,18000101,PRCP,0,,,E,\n",
        "     .          .     .       .\n",
        "     .          .     .       .\n",
        "     .          .     .       .\n",
        "```"
      ],
      "metadata": {
        "id": "t51kvqQRZ9uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    stationID = fields[0]\n",
        "    entryType = fields[2]\n",
        "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0 # convert to F from C\n",
        "    return (stationID, entryType, temperature)\n",
        "\n",
        "lines = sc.textFile(\"file:///SparkCourse/1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
        "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
        "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
        "results = minTemps.collect();\n",
        "\n",
        "for result in results:\n",
        "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))"
      ],
      "metadata": {
        "id": "Fn1klz5kYoBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### max-temperatures.py"
      ],
      "metadata": {
        "id": "FffpiqXPYrX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    stationID = fields[0]\n",
        "    entryType = fields[2]\n",
        "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
        "    return (stationID, entryType, temperature)\n",
        "\n",
        "lines = sc.textFile(\"file:///SparkCourse/1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "maxTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
        "stationTemps = maxTemps.map(lambda x: (x[0], x[2]))\n",
        "maxTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
        "results = maxTemps.collect();\n",
        "\n",
        "for result in results:\n",
        "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))"
      ],
      "metadata": {
        "id": "Fl-HuhrKYuVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word-count.py"
      ],
      "metadata": {
        "id": "fateCCdVY9Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "input = sc.textFile(\"file:///sparkcourse/book.txt\")\n",
        "words = input.flatMap(lambda x: x.split())\n",
        "wordCounts = words.countByValue()\n",
        "\n",
        "for word, count in wordCounts.items():\n",
        "    cleanWord = word.encode('ascii', 'ignore')\n",
        "    if (cleanWord):\n",
        "        print(cleanWord.decode() + \" \" + str(count))"
      ],
      "metadata": {
        "id": "j66XMiBbZBHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word-count-better-sorted.py"
      ],
      "metadata": {
        "id": "J_m5b_FEY-ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "def normalizeWords(text):\n",
        "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "input = sc.textFile(\"file:///sparkcourse/book.txt\")\n",
        "words = input.flatMap(normalizeWords)\n",
        "\n",
        "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey()\n",
        "results = wordCountsSorted.collect()\n",
        "\n",
        "for result in results:\n",
        "    count = str(result[0])\n",
        "    word = result[1].encode('ascii', 'ignore')\n",
        "    if (word):\n",
        "        print(word.decode() + \":\\t\\t\" + count)"
      ],
      "metadata": {
        "id": "gIIiznFTZAyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### total-spent-by-customer-sorted.py"
      ],
      "metadata": {
        "id": "hoSDMkeJYxjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(customer id, item id, pay amount)"
      ],
      "metadata": {
        "id": "Y1SbGJH7imO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomerSorted\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def extractCustomerPricePairs(line):\n",
        "    fields = line.split(',')\n",
        "    return (int(fields[0]), float(fields[2]))\n",
        "\n",
        "input = sc.textFile(\"file:///sparkcourse/customer-orders.csv\")\n",
        "mappedInput = input.map(extractCustomerPricePairs)\n",
        "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "#Changed for Python 3 compatibility:\n",
        "#flipped = totalByCustomer.map(lambda (x,y):(y,x))\n",
        "flipped = totalByCustomer.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "totalByCustomerSorted = flipped.sortByKey()\n",
        "\n",
        "results = totalByCustomerSorted.collect();\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "8z_wk_V_Y3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3\n",
        "SparkSQL, DataFrames, and DataSets"
      ],
      "metadata": {
        "id": "yvBUxcKhZKwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spark-sql.py"
      ],
      "metadata": {
        "id": "yoYwtck_ZOPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(id, name, age, number_of_friends)"
      ],
      "metadata": {
        "id": "UNLzOPGnSLfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "def mapper(line):\n",
        "    fields = line.split(',')\n",
        "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), \\\n",
        "               age=int(fields[2]), numFriends=int(fields[3]))\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"fakefriends.csv\")\n",
        "people = lines.map(mapper)\n",
        "\n",
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people).cache()\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
        "for teen in teenagers.collect():\n",
        "  print(teen)\n",
        "\n",
        "# We can also use functions instead of SQL queries:\n",
        "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "FoIVMHNpZqr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spark-sql-dataframe.py"
      ],
      "metadata": {
        "id": "kZFMITSKZOLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "people = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
        "    .csv(\"file:///SparkCourse/fakefriends-header.csv\")\n",
        "\n",
        "print(\"Here is our inferred schema:\")\n",
        "people.printSchema()\n",
        "\n",
        "print(\"Let's display the name column:\")\n",
        "people.select(\"name\").show()\n",
        "\n",
        "print(\"Filter out anyone over 21:\")\n",
        "people.filter(people.age < 21).show()\n",
        "\n",
        "print(\"Group by age\")\n",
        "people.groupBy(\"age\").count().show()\n",
        "\n",
        "print(\"Make everyone 10 years older:\")\n",
        "people.select(people.name, people.age + 10).show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "LIwELBzeZqVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### friends-by-age-dataframe.py"
      ],
      "metadata": {
        "id": "u9QgGwujZOAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions as func\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FriendsByAge\").getOrCreate()\n",
        "\n",
        "lines = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"file:///SparkCourse/fakefriends-header.csv\")\n",
        "\n",
        "# Select only age and numFriends columns\n",
        "friendsByAge = lines.select(\"age\", \"friends\")\n",
        "\n",
        "# From friendsByAge we group by \"age\" and then compute average\n",
        "friendsByAge.groupBy(\"age\").avg(\"friends\").show()\n",
        "\n",
        "# Sorted\n",
        "friendsByAge.groupBy(\"age\").avg(\"friends\").sort(\"age\").show()\n",
        "\n",
        "# Formatted more nicely\n",
        "friendsByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"), 2)).sort(\"age\").show()\n",
        "\n",
        "# With a custom column name\n",
        "friendsByAge.groupBy(\"age\").agg(func.round(func.avg(\"friends\"), 2)\n",
        "  .alias(\"friends_avg\")).sort(\"age\").show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "jY3FRQLjZph-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word-count-better-sorted-dataframe.py"
      ],
      "metadata": {
        "id": "meDz1I5MZmaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "\n",
        "# Read each line of my book into a dataframe\n",
        "inputDF = spark.read.text(\"file:///SparkCourse/book.txt\")\n",
        "\n",
        "# Split using a regular expression that extracts words\n",
        "words = inputDF.select(func.explode(func.split(inputDF.value, \"\\\\W+\")).alias(\"word\"))\n",
        "wordsWithoutEmptyString = words.filter(words.word != \"\")\n",
        "\n",
        "# Normalize everything to lowercase\n",
        "lowercaseWords = wordsWithoutEmptyString.select(func.lower(wordsWithoutEmptyString.word).alias(\"word\"))\n",
        "\n",
        "# Count up the occurrences of each word\n",
        "wordCounts = lowercaseWords.groupBy(\"word\").count()\n",
        "\n",
        "# Sort by counts\n",
        "wordCountsSorted = wordCounts.sort(\"count\")\n",
        "\n",
        "# Show the results.\n",
        "wordCountsSorted.show(wordCountsSorted.count())\n"
      ],
      "metadata": {
        "id": "fMkt-w1NZrZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### min-temperatures-dataframe.py"
      ],
      "metadata": {
        "id": "BkajnZEIZbJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MinTemperatures\").getOrCreate()\n",
        "\n",
        "# schema dibutuhkan karena tidak dimiliki header untuk data di csv\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"stationID\", StringType(), True), \\\n",
        "                     StructField(\"date\", IntegerType(), True), \\\n",
        "                     StructField(\"measure_type\", StringType(), True), \\\n",
        "                     StructField(\"temperature\", FloatType(), True)]) # <- langsung dinamain, walaupun belum tentu temp\n",
        "\n",
        "# // Read the file as dataframe\n",
        "df = spark.read.schema(schema).csv(\"file:///SparkCourse/1800.csv\")\n",
        "df.printSchema()\n",
        "\n",
        "# Filter out all but TMIN entries\n",
        "minTemps = df.filter(df.measure_type == \"TMIN\")\n",
        "\n",
        "# Select only stationID and temperature\n",
        "stationTemps = minTemps.select(\"stationID\", \"temperature\")\n",
        "\n",
        "# Aggregate to find minimum temperature for every station\n",
        "minTempsByStation = stationTemps.groupBy(\"stationID\").min(\"temperature\")\n",
        "minTempsByStation.show()\n",
        "\n",
        "# Convert temperature to fahrenheit and sort the dataset\n",
        "minTempsByStationF = minTempsByStation.withColumn(\"temperature\",\n",
        "                                                  func.round(func.col(\"min(temperature)\") * 0.1 * (9.0 / 5.0) + 32.0, 2))\\\n",
        "                                                  .select(\"stationID\", \"temperature\").sort(\"temperature\")\n",
        "\n",
        "### witColumn berguna untuk membuat kolom baru. Nama kolom lama\n",
        "###  ternyata \"min(temperature)\" otomatis dari pyspark pada line 24\n",
        "### https://stackoverflow.com/questions/39050248/how-to-modify-transform-the-column-of-a-dataframe\n",
        "\n",
        "# Collect, format, and print the results\n",
        "results = minTempsByStationF.collect()\n",
        "\n",
        "for result in results:\n",
        "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "gbJa50WkZp9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### total-spent-customer-sorted-dataframe.py"
      ],
      "metadata": {
        "id": "PpMyYYsjZOSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TotalSpentByCustomer\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Create schema when reading customer-orders\n",
        "customerOrderSchema = StructType([ \\\n",
        "                                  StructField(\"cust_id\", IntegerType(), True),\n",
        "                                  StructField(\"item_id\", IntegerType(), True),\n",
        "                                  StructField(\"amount_spent\", FloatType(), True)\n",
        "                                  ])\n",
        "\n",
        "# Load up the data into spark dataset\n",
        "customersDF = spark.read.schema(customerOrderSchema).csv(\"file:///SparkCourse/customer-orders.csv\")\n",
        "\n",
        "totalByCustomer = customersDF.groupBy(\"cust_id\").agg(func.round(func.sum(\"amount_spent\"), 2) \\\n",
        "                                      .alias(\"total_spent\"))\n",
        "\n",
        "totalByCustomerSorted = totalByCustomer.sort(\"total_spent\")\n",
        "\n",
        "totalByCustomerSorted.show(totalByCustomerSorted.count())\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "pnP_HJYVXmz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4\n",
        "AdvancedExamples of Spark Programs"
      ],
      "metadata": {
        "id": "YY6vS_92aAy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### popular-movies-dataframe.py"
      ],
      "metadata": {
        "id": "pGcqFYehbFCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
        "\n",
        "# Create schema when reading u.data\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"userID\", IntegerType(), True), \\\n",
        "                     StructField(\"movieID\", IntegerType(), True), \\\n",
        "                     StructField(\"rating\", IntegerType(), True), \\\n",
        "                     StructField(\"timestamp\", LongType(), True)])\n",
        "\n",
        "# Load up movie data as dataframe\n",
        "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"file:///SparkCourse/ml-100k/u.data\")\n",
        "\n",
        "# Some SQL-style magic to sort all movies by popularity in one line!\n",
        "topMovieIDs = moviesDF.groupBy(\"movieID\").count().orderBy(func.desc(\"count\"))\n",
        "\n",
        "# Grab the top 10\n",
        "topMovieIDs.show(10)\n",
        "\n",
        "# Stop the session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "wkFGX0qbbI3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### popular-movies-nice-dataframe.py"
      ],
      "metadata": {
        "id": "q6xrko3HbGht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Sep  7 15:28:00 2020\n",
        "\n",
        "@author: Frank\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
        "import codecs\n",
        "\n",
        "def loadMovieNames():\n",
        "    movieNames = {}\n",
        "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
        "    with codecs.open(\"E:/SparkCourse/ml-100k/u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            fields = line.split('|')\n",
        "            movieNames[int(fields[0])] = fields[1]\n",
        "    return movieNames\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
        "\n",
        "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
        "\n",
        "# Create schema when reading u.data\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"userID\", IntegerType(), True), \\\n",
        "                     StructField(\"movieID\", IntegerType(), True), \\\n",
        "                     StructField(\"rating\", IntegerType(), True), \\\n",
        "                     StructField(\"timestamp\", LongType(), True)])\n",
        "\n",
        "# Load up movie data as dataframe\n",
        "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"file:///SparkCourse/ml-100k/u.data\")\n",
        "\n",
        "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
        "\n",
        "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
        "def lookupName(movieID):\n",
        "    return nameDict.value[movieID]\n",
        "\n",
        "lookupNameUDF = func.udf(lookupName)\n",
        "\n",
        "# Add a movieTitle column using our new udf\n",
        "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(func.col(\"movieID\")))\n",
        "\n",
        "# Sort the results\n",
        "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
        "\n",
        "# Grab the top 10\n",
        "sortedMoviesWithNames.show(10, False)\n",
        "\n",
        "# Stop the session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "YmA6_gsjbJTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### most-popular-superhero-dataframe.py"
      ],
      "metadata": {
        "id": "jRLYer72a4i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MostPopularSuperhero\").getOrCreate()\n",
        "\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"id\", IntegerType(), True), \\\n",
        "                     StructField(\"name\", StringType(), True)])\n",
        "\n",
        "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"file:///SparkCourse/Marvel-names.txt\")\n",
        "\n",
        "lines = spark.read.text(\"file:///SparkCourse/Marvel-graph.txt\")\n",
        "\n",
        "# Small tweak vs. what's shown in the video: we trim each line of whitespace as that could\n",
        "# throw off the counts.\n",
        "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
        "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
        "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
        "\n",
        "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
        "\n",
        "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
        "\n",
        "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")"
      ],
      "metadata": {
        "id": "PctjEGfLa4QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### most-obscure-superheroes.py"
      ],
      "metadata": {
        "id": "n7Av2rJoa10G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MostObscureSuperheroes\").getOrCreate()\n",
        "\n",
        "schema = StructType([ \\\n",
        "                     StructField(\"id\", IntegerType(), True), \\\n",
        "                     StructField(\"name\", StringType(), True)])\n",
        "\n",
        "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"file:///SparkCourse/Marvel-names.txt\")\n",
        "\n",
        "lines = spark.read.text(\"file:///SparkCourse/Marvel-graph.txt\")\n",
        "\n",
        "# Small tweak vs. what's shown in the video: we trim whitespace from each line as this\n",
        "# could throw the counts off by one.\n",
        "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
        "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
        "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
        "\n",
        "minConnectionCount = connections.agg(func.min(\"connections\")).first()[0]\n",
        "\n",
        "minConnections = connections.filter(func.col(\"connections\") == minConnectionCount)\n",
        "\n",
        "minConnectionsWithNames = minConnections.join(names, \"id\")\n",
        "\n",
        "print(\"The following characters have only \" + str(minConnectionCount) + \" connection(s):\")\n",
        "\n",
        "minConnectionsWithNames.select(\"name\").show()"
      ],
      "metadata": {
        "id": "mBFELD01aovE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### degrees-of-separation.py"
      ],
      "metadata": {
        "id": "8JMmDJSZaCNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boilerplate stuff:\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "# The characters we wish to find the degree of separation between:\n",
        "startCharacterID = 5306 #SpiderMan\n",
        "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
        "\n",
        "# Our accumulator, used to signal when we find the target character during\n",
        "# our BFS traversal.\n",
        "hitCounter = sc.accumulator(0)\n",
        "\n",
        "def convertToBFS(line):\n",
        "    fields = line.split()\n",
        "    heroID = int(fields[0])\n",
        "    connections = []\n",
        "    for connection in fields[1:]:\n",
        "        connections.append(int(connection))\n",
        "\n",
        "    color = 'WHITE'\n",
        "    distance = 9999\n",
        "\n",
        "    if (heroID == startCharacterID):\n",
        "        color = 'GRAY'\n",
        "        distance = 0\n",
        "\n",
        "    return (heroID, (connections, distance, color))\n",
        "\n",
        "\n",
        "def createStartingRdd():\n",
        "    inputFile = sc.textFile(\"file:///sparkcourse/marvel-graph.txt\")\n",
        "    return inputFile.map(convertToBFS)\n",
        "\n",
        "def bfsMap(node):\n",
        "    characterID = node[0]\n",
        "    data = node[1]\n",
        "    connections = data[0]\n",
        "    distance = data[1]\n",
        "    color = data[2]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    #If this node needs to be expanded...\n",
        "    if (color == 'GRAY'):\n",
        "        for connection in connections:\n",
        "            newCharacterID = connection\n",
        "            newDistance = distance + 1\n",
        "            newColor = 'GRAY'\n",
        "            if (targetCharacterID == connection):\n",
        "                hitCounter.add(1)\n",
        "\n",
        "            newEntry = (newCharacterID, ([], newDistance, newColor))\n",
        "            results.append(newEntry)\n",
        "\n",
        "        #We've processed this node, so color it black\n",
        "        color = 'BLACK'\n",
        "\n",
        "    #Emit the input node so we don't lose it.\n",
        "    results.append( (characterID, (connections, distance, color)) )\n",
        "    return results\n",
        "\n",
        "def bfsReduce(data1, data2):\n",
        "    edges1 = data1[0]\n",
        "    edges2 = data2[0]\n",
        "    distance1 = data1[1]\n",
        "    distance2 = data2[1]\n",
        "    color1 = data1[2]\n",
        "    color2 = data2[2]\n",
        "\n",
        "    distance = 9999\n",
        "    color = color1\n",
        "    edges = []\n",
        "\n",
        "    # See if one is the original node with its connections.\n",
        "    # If so preserve them.\n",
        "    if (len(edges1) > 0):\n",
        "        edges.extend(edges1)\n",
        "    if (len(edges2) > 0):\n",
        "        edges.extend(edges2)\n",
        "\n",
        "    # Preserve minimum distance\n",
        "    if (distance1 < distance):\n",
        "        distance = distance1\n",
        "\n",
        "    if (distance2 < distance):\n",
        "        distance = distance2\n",
        "\n",
        "    # Preserve darkest color\n",
        "    if (color1 == 'WHITE' and (color2 == 'GRAY' or color2 == 'BLACK')):\n",
        "        color = color2\n",
        "\n",
        "    if (color1 == 'GRAY' and color2 == 'BLACK'):\n",
        "        color = color2\n",
        "\n",
        "    if (color2 == 'WHITE' and (color1 == 'GRAY' or color1 == 'BLACK')):\n",
        "        color = color1\n",
        "\n",
        "    if (color2 == 'GRAY' and color1 == 'BLACK'):\n",
        "        color = color1\n",
        "\n",
        "    return (edges, distance, color)\n",
        "\n",
        "\n",
        "#Main program here:\n",
        "iterationRdd = createStartingRdd()\n",
        "\n",
        "for iteration in range(0, 10):\n",
        "    print(\"Running BFS iteration# \" + str(iteration+1))\n",
        "\n",
        "    # Create new vertices as needed to darken or reduce distances in the\n",
        "    # reduce stage. If we encounter the node we're looking for as a GRAY\n",
        "    # node, increment our accumulator to signal that we're done.\n",
        "    mapped = iterationRdd.flatMap(bfsMap)\n",
        "\n",
        "    # Note that mapped.count() action here forces the RDD to be evaluated, and\n",
        "    # that's the only reason our accumulator is actually updated.\n",
        "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
        "\n",
        "    if (hitCounter.value > 0):\n",
        "        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n",
        "            + \" different direction(s).\")\n",
        "        break\n",
        "\n",
        "    # Reducer combines data for each character ID, preserving the darkest\n",
        "    # color and shortest path.\n",
        "    iterationRdd = mapped.reduceByKey(bfsReduce)\n"
      ],
      "metadata": {
        "id": "C6UocH05Z81y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### movie-similarities-dataframe.py"
      ],
      "metadata": {
        "id": "whd_6bXhbMRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
        "import sys\n",
        "\n",
        "def computeCosineSimilarity(spark, data):\n",
        "    # Compute xx, xy and yy columns\n",
        "    pairScores = data \\\n",
        "      .withColumn(\"xx\", func.col(\"rating1\") * func.col(\"rating1\")) \\\n",
        "      .withColumn(\"yy\", func.col(\"rating2\") * func.col(\"rating2\")) \\\n",
        "      .withColumn(\"xy\", func.col(\"rating1\") * func.col(\"rating2\"))\n",
        "\n",
        "    # Compute numerator, denominator and numPairs columns\n",
        "    calculateSimilarity = pairScores \\\n",
        "      .groupBy(\"movie1\", \"movie2\") \\\n",
        "      .agg( \\\n",
        "        func.sum(func.col(\"xy\")).alias(\"numerator\"), \\\n",
        "        (func.sqrt(func.sum(func.col(\"xx\"))) * func.sqrt(func.sum(func.col(\"yy\")))).alias(\"denominator\"), \\\n",
        "        func.count(func.col(\"xy\")).alias(\"numPairs\")\n",
        "      )\n",
        "\n",
        "    # Calculate score and select only needed columns (movie1, movie2, score, numPairs)\n",
        "    result = calculateSimilarity \\\n",
        "      .withColumn(\"score\", \\\n",
        "        func.when(func.col(\"denominator\") != 0, func.col(\"numerator\") / func.col(\"denominator\")) \\\n",
        "          .otherwise(0) \\\n",
        "      ).select(\"movie1\", \"movie2\", \"score\", \"numPairs\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# Get movie name by given movie id\n",
        "def getMovieName(movieNames, movieId):\n",
        "    result = movieNames.filter(func.col(\"movieID\") == movieId) \\\n",
        "        .select(\"movieTitle\").collect()[0]\n",
        "\n",
        "    return result[0]\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MovieSimilarities\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "movieNamesSchema = StructType([ \\\n",
        "                               StructField(\"movieID\", IntegerType(), True), \\\n",
        "                               StructField(\"movieTitle\", StringType(), True) \\\n",
        "                               ])\n",
        "\n",
        "moviesSchema = StructType([ \\\n",
        "                     StructField(\"userID\", IntegerType(), True), \\\n",
        "                     StructField(\"movieID\", IntegerType(), True), \\\n",
        "                     StructField(\"rating\", IntegerType(), True), \\\n",
        "                     StructField(\"timestamp\", LongType(), True)])\n",
        "\n",
        "\n",
        "# Create a broadcast dataset of movieID and movieTitle.\n",
        "# Apply ISO-885901 charset\n",
        "movieNames = spark.read \\\n",
        "      .option(\"sep\", \"|\") \\\n",
        "      .option(\"charset\", \"ISO-8859-1\") \\\n",
        "      .schema(movieNamesSchema) \\\n",
        "      .csv(\"file:///SparkCourse/ml-100k/u.item\")\n",
        "\n",
        "# Load up movie data as dataset\n",
        "movies = spark.read \\\n",
        "      .option(\"sep\", \"\\t\") \\\n",
        "      .schema(moviesSchema) \\\n",
        "      .csv(\"file:///SparkCourse/ml-100k/u.data\")\n",
        "\n",
        "\n",
        "ratings = movies.select(\"userId\", \"movieId\", \"rating\")\n",
        "\n",
        "# Emit every movie rated together by the same user.\n",
        "# Self-join to find every combination.\n",
        "# Select movie pairs and rating pairs\n",
        "moviePairs = ratings.alias(\"ratings1\") \\\n",
        "      .join(ratings.alias(\"ratings2\"), (func.col(\"ratings1.userId\") == func.col(\"ratings2.userId\")) \\\n",
        "            & (func.col(\"ratings1.movieId\") < func.col(\"ratings2.movieId\"))) \\\n",
        "      .select(func.col(\"ratings1.movieId\").alias(\"movie1\"), \\\n",
        "        func.col(\"ratings2.movieId\").alias(\"movie2\"), \\\n",
        "        func.col(\"ratings1.rating\").alias(\"rating1\"), \\\n",
        "        func.col(\"ratings2.rating\").alias(\"rating2\"))\n",
        "\n",
        "\n",
        "moviePairSimilarities = computeCosineSimilarity(spark, moviePairs).cache()\n",
        "\n",
        "if (len(sys.argv) > 1):\n",
        "    scoreThreshold = 0.97\n",
        "    coOccurrenceThreshold = 50.0\n",
        "\n",
        "    movieID = int(sys.argv[1])\n",
        "\n",
        "    # Filter for movies with this sim that are \"good\" as defined by\n",
        "    # our quality thresholds above\n",
        "    filteredResults = moviePairSimilarities.filter( \\\n",
        "        ((func.col(\"movie1\") == movieID) | (func.col(\"movie2\") == movieID)) & \\\n",
        "          (func.col(\"score\") > scoreThreshold) & (func.col(\"numPairs\") > coOccurrenceThreshold))\n",
        "\n",
        "    # Sort by quality score.\n",
        "    results = filteredResults.sort(func.col(\"score\").desc()).take(10)\n",
        "\n",
        "    print (\"Top 10 similar movies for \" + getMovieName(movieNames, movieID))\n",
        "\n",
        "    for result in results:\n",
        "        # Display the similarity result that isn't the movie we're looking at\n",
        "        similarMovieID = result.movie1\n",
        "        if (similarMovieID == movieID):\n",
        "          similarMovieID = result.movie2\n",
        "\n",
        "        print(getMovieName(movieNames, similarMovieID) + \"\\tscore: \" \\\n",
        "              + str(result.score) + \"\\tstrength: \" + str(result.numPairs))"
      ],
      "metadata": {
        "id": "1soV1zZSbOde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5"
      ],
      "metadata": {
        "id": "4Rg6lMaaaCG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MovieSimilarities1M.py"
      ],
      "metadata": {
        "id": "omUNxRonbovi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from math import sqrt\n",
        "\n",
        "def loadMovieNames():\n",
        "    movieNames = {}\n",
        "    with open(\"movies.dat\",  encoding='ascii', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            fields = line.split(\"::\")\n",
        "            movieNames[int(fields[0])] = fields[1]\n",
        "    return movieNames\n",
        "\n",
        "def makePairs( userRatings ):\n",
        "    ratings = userRatings[1]\n",
        "    (movie1, rating1) = ratings[0]\n",
        "    (movie2, rating2) = ratings[1]\n",
        "    return ((movie1, movie2), (rating1, rating2))\n",
        "\n",
        "def filterDuplicates( userRatings ):\n",
        "    ratings = userRatings[1]\n",
        "    (movie1, rating1) = ratings[0]\n",
        "    (movie2, rating2) = ratings[1]\n",
        "    return movie1 < movie2\n",
        "\n",
        "def computeCosineSimilarity(ratingPairs):\n",
        "    numPairs = 0\n",
        "    sum_xx = sum_yy = sum_xy = 0\n",
        "    for ratingX, ratingY in ratingPairs:\n",
        "        sum_xx += ratingX * ratingX\n",
        "        sum_yy += ratingY * ratingY\n",
        "        sum_xy += ratingX * ratingY\n",
        "        numPairs += 1\n",
        "\n",
        "    numerator = sum_xy\n",
        "    denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
        "\n",
        "    score = 0\n",
        "    if (denominator):\n",
        "        score = (numerator / (float(denominator)))\n",
        "\n",
        "    return (score, numPairs)\n",
        "\n",
        "\n",
        "conf = SparkConf()\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "print(\"\\nLoading movie names...\")\n",
        "nameDict = loadMovieNames()\n",
        "\n",
        "data = sc.textFile(\"s3n://sundog-spark/ml-1m/ratings.dat\")\n",
        "\n",
        "# Map ratings to key / value pairs: user ID => movie ID, rating\n",
        "ratings = data.map(lambda l: l.split(\"::\")).map(lambda l: (int(l[0]), (int(l[1]), float(l[2]))))\n",
        "\n",
        "# Emit every movie rated together by the same user.\n",
        "# Self-join to find every combination.\n",
        "ratingsPartitioned = ratings.partitionBy(100)\n",
        "joinedRatings = ratingsPartitioned.join(ratingsPartitioned)\n",
        "\n",
        "# At this point our RDD consists of userID => ((movieID, rating), (movieID, rating))\n",
        "\n",
        "# Filter out duplicate pairs\n",
        "uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)\n",
        "\n",
        "# Now key by (movie1, movie2) pairs.\n",
        "moviePairs = uniqueJoinedRatings.map(makePairs).partitionBy(100)\n",
        "\n",
        "# We now have (movie1, movie2) => (rating1, rating2)\n",
        "# Now collect all ratings for each movie pair and compute similarity\n",
        "moviePairRatings = moviePairs.groupByKey()\n",
        "\n",
        "# We now have (movie1, movie2) = > (rating1, rating2), (rating1, rating2) ...\n",
        "# Can now compute similarities.\n",
        "moviePairSimilarities = moviePairRatings.mapValues(computeCosineSimilarity).persist()\n",
        "\n",
        "# Save the results if desired\n",
        "moviePairSimilarities.sortByKey()\n",
        "moviePairSimilarities.saveAsTextFile(\"movie-sims\")\n",
        "\n",
        "# Extract similarities for the movie we care about that are \"good\".\n",
        "if (len(sys.argv) > 1):\n",
        "\n",
        "    scoreThreshold = 0.97\n",
        "    coOccurenceThreshold = 50\n",
        "\n",
        "    movieID = int(sys.argv[1])\n",
        "\n",
        "    # Filter for movies with this sim that are \"good\" as defined by\n",
        "    # our quality thresholds above\n",
        "    filteredResults = moviePairSimilarities.filter(lambda pairSim: \\\n",
        "        (pairSim[0][0] == movieID or pairSim[0][1] == movieID) \\\n",
        "        and pairSim[1][0] > scoreThreshold and pairSim[1][1] > coOccurenceThreshold)\n",
        "\n",
        "    # Sort by quality score.\n",
        "    results = filteredResults.map(lambda pairSim: (pairSim[1], pairSim[0])).sortByKey(ascending = False).take(10)\n",
        "\n",
        "    print(\"Top 10 similar movies for \" + nameDict[movieID])\n",
        "    for result in results:\n",
        "        (sim, pair) = result\n",
        "        # Display the similarity result that isn't the movie we're looking at\n",
        "        similarMovieID = pair[0]\n",
        "        if (similarMovieID == movieID):\n",
        "            similarMovieID = pair[1]\n",
        "        print(nameDict[similarMovieID] + \"\\tscore: \" + str(sim[0]) + \"\\tstrength: \" + str(sim[1]))"
      ],
      "metadata": {
        "id": "1JV23SWEZ-A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 6"
      ],
      "metadata": {
        "id": "3zseH-amaCtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### movie-recommendations-als-dataframe.py"
      ],
      "metadata": {
        "id": "Fma0x54MbzLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
        "from pyspark.ml.recommendation import ALS\n",
        "import sys\n",
        "import codecs\n",
        "\n",
        "def loadMovieNames():\n",
        "    movieNames = {}\n",
        "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
        "    with codecs.open(\"E:/SparkCourse/ml-100k/u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            fields = line.split('|')\n",
        "            movieNames[int(fields[0])] = fields[1]\n",
        "    return movieNames\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ALSExample\").getOrCreate()\n",
        "\n",
        "moviesSchema = StructType([ \\\n",
        "                     StructField(\"userID\", IntegerType(), True), \\\n",
        "                     StructField(\"movieID\", IntegerType(), True), \\\n",
        "                     StructField(\"rating\", IntegerType(), True), \\\n",
        "                     StructField(\"timestamp\", LongType(), True)])\n",
        "\n",
        "names = loadMovieNames()\n",
        "\n",
        "ratings = spark.read.option(\"sep\", \"\\t\").schema(moviesSchema) \\\n",
        "    .csv(\"file:///SparkCourse/ml-100k/u.data\")\n",
        "\n",
        "print(\"Training recommendation model...\")\n",
        "\n",
        "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userID\").setItemCol(\"movieID\") \\\n",
        "    .setRatingCol(\"rating\")\n",
        "\n",
        "model = als.fit(ratings)\n",
        "\n",
        "# Manually construct a dataframe of the user ID's we want recs for\n",
        "userID = int(sys.argv[1])\n",
        "userSchema = StructType([StructField(\"userID\", IntegerType(), True)])\n",
        "users = spark.createDataFrame([[userID,]], userSchema)\n",
        "\n",
        "recommendations = model.recommendForUserSubset(users, 10).collect()\n",
        "\n",
        "print(\"Top 10 recommendations for user ID \" + str(userID))\n",
        "\n",
        "for userRecs in recommendations:\n",
        "    myRecs = userRecs[1]  #userRecs is (userID, [Row(movieId, rating), Row(movieID, rating)...])\n",
        "    for rec in myRecs: #my Recs is just the column of recs for the user\n",
        "        movie = rec[0] #For each rec in the list, extract the movie ID and rating\n",
        "        rating = rec[1]\n",
        "        movieName = names[movie]\n",
        "        print(movieName + str(rating))"
      ],
      "metadata": {
        "id": "OV6iWtSebue0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### real-estate.py"
      ],
      "metadata": {
        "id": "fCEM7GnMb1SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
        "    spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()\n",
        "\n",
        "\n",
        "    # Load up data as dataframe\n",
        "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
        "        .csv(\"file:///SparkCourse/realestate.csv\")\n",
        "\n",
        "    assembler = VectorAssembler().setInputCols([\"HouseAge\", \"DistanceToMRT\", \\\n",
        "                               \"NumberConvenienceStores\"]).setOutputCol(\"features\")\n",
        "\n",
        "    df = assembler.transform(data).select(\"PriceOfUnitArea\", \"features\")\n",
        "\n",
        "    # Let's split our data into training data and testing data\n",
        "    trainTest = df.randomSplit([0.5, 0.5])\n",
        "    trainingDF = trainTest[0]\n",
        "    testDF = trainTest[1]\n",
        "\n",
        "    # Now create our decision tree\n",
        "    dtr = DecisionTreeRegressor().setFeaturesCol(\"features\").setLabelCol(\"PriceOfUnitArea\")\n",
        "\n",
        "    # Train the model using our training data\n",
        "    model = dtr.fit(trainingDF)\n",
        "\n",
        "    # Now see if we can predict values in our test data.\n",
        "    # Generate predictions using our decision tree model for all features in our\n",
        "    # test dataframe:\n",
        "    fullPredictions = model.transform(testDF).cache()\n",
        "\n",
        "    # Extract the predictions and the \"known\" correct labels.\n",
        "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
        "    labels = fullPredictions.select(\"PriceOfUnitArea\").rdd.map(lambda x: x[0])\n",
        "\n",
        "    # Zip them together\n",
        "    predictionAndLabel = predictions.zip(labels).collect()\n",
        "\n",
        "    # Print out the predicted and actual values for each point\n",
        "    for prediction in predictionAndLabel:\n",
        "      print(prediction)\n",
        "\n",
        "\n",
        "    # Stop the session\n",
        "    spark.stop()"
      ],
      "metadata": {
        "id": "WBZjtxgFb32v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spark-linear-regression.py"
      ],
      "metadata": {
        "id": "TxtSWHkMb39B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
        "    spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()\n",
        "\n",
        "    # Load up our data and convert it to the format MLLib expects.\n",
        "    inputLines = spark.sparkContext.textFile(\"regression.txt\")\n",
        "    data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))\n",
        "\n",
        "    # Convert this RDD to a DataFrame\n",
        "    colNames = [\"label\", \"features\"]\n",
        "    df = data.toDF(colNames)\n",
        "\n",
        "    # Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.\n",
        "    # Perhaps you're importing data from a real database. Or you are using structured streaming\n",
        "    # to get your data.\n",
        "\n",
        "    # Let's split our data into training data and testing data\n",
        "    trainTest = df.randomSplit([0.5, 0.5])\n",
        "    trainingDF = trainTest[0]\n",
        "    testDF = trainTest[1]\n",
        "\n",
        "    # Now create our linear regression model\n",
        "    lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "    # Train the model using our training data\n",
        "    model = lir.fit(trainingDF)\n",
        "\n",
        "    # Now see if we can predict values in our test data.\n",
        "    # Generate predictions using our linear regression model for all features in our\n",
        "    # test dataframe:\n",
        "    fullPredictions = model.transform(testDF).cache()\n",
        "\n",
        "    # Extract the predictions and the \"known\" correct labels.\n",
        "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
        "    labels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
        "\n",
        "    # Zip them together\n",
        "    predictionAndLabel = predictions.zip(labels).collect()\n",
        "\n",
        "    # Print out the predicted and actual values for each point\n",
        "    for prediction in predictionAndLabel:\n",
        "      print(prediction)\n",
        "\n",
        "\n",
        "    # Stop the session\n",
        "    spark.stop()"
      ],
      "metadata": {
        "id": "eqSZy8Qmb6eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 7"
      ],
      "metadata": {
        "id": "KnBEI01mcKML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### structured-streaming.py"
      ],
      "metadata": {
        "id": "X2AkgtqAgYj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Dec 18 09:15:05 2019\n",
        "\n",
        "@author: Frank\n",
        "\"\"\"\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import Row, SparkSession\n",
        "\n",
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "# Create a SparkSession (the config bit is only for Windows!)\n",
        "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"StructuredStreaming\").getOrCreate()\n",
        "\n",
        "# Monitor the logs directory for new log data, and read in the raw lines as accessLines\n",
        "accessLines = spark.readStream.text(\"logs\")\n",
        "\n",
        "# Parse out the common log format to a DataFrame\n",
        "contentSizeExp = r'\\s(\\d+)$'\n",
        "statusExp = r'\\s(\\d{3})\\s'\n",
        "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
        "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
        "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
        "\n",
        "logsDF = accessLines.select(regexp_extract('value', hostExp, 1).alias('host'),\n",
        "                         regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
        "                         regexp_extract('value', generalExp, 1).alias('method'),\n",
        "                         regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
        "                         regexp_extract('value', generalExp, 3).alias('protocol'),\n",
        "                         regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
        "                         regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))\n",
        "\n",
        "# Keep a running count of every access by status code\n",
        "statusCountsDF = logsDF.groupBy(logsDF.status).count()\n",
        "\n",
        "# Kick off our streaming query, dumping results to the console\n",
        "query = ( statusCountsDF.writeStream.outputMode(\"complete\").format(\"console\").queryName(\"counts\").start() )\n",
        "\n",
        "# Run forever until terminated\n",
        "query.awaitTermination()\n",
        "\n",
        "# Cleanly shut down the session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "1fwBdRgogYgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top-urls.py"
      ],
      "metadata": {
        "id": "H5BJFlj8gYcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pyspark.sql.functions as func\n",
        "\n",
        "# Create a SparkSession (the config bit is only for Windows!)\n",
        "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()\n",
        "\n",
        "# Monitor the logs directory for new log data, and read in the raw lines as accessLines\n",
        "accessLines = spark.readStream.text(\"logs\")\n",
        "\n",
        "# Parse out the common log format to a DataFrame\n",
        "contentSizeExp = r'\\s(\\d+)$'\n",
        "statusExp = r'\\s(\\d{3})\\s'\n",
        "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
        "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
        "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
        "\n",
        "logsDF = accessLines.select(func.regexp_extract('value', hostExp, 1).alias('host'),\n",
        "                         func.regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
        "                         func.regexp_extract('value', generalExp, 1).alias('method'),\n",
        "                         func.regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
        "                         func.regexp_extract('value', generalExp, 3).alias('protocol'),\n",
        "                         func.regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
        "                         func.regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))\n",
        "\n",
        "logsDF2 = logsDF.withColumn(\"eventTime\", func.current_timestamp())\n",
        "\n",
        "# Keep a running count of endpoints\n",
        "endpointCounts = logsDF2.groupBy(func.window(func.col(\"eventTime\"), \\\n",
        "      \"30 seconds\", \"10 seconds\"), func.col(\"endpoint\")).count()\n",
        "\n",
        "sortedEndpointCounts = endpointCounts.orderBy(func.col(\"count\").desc())\n",
        "\n",
        "# Display the stream to the console\n",
        "query = sortedEndpointCounts.writeStream.outputMode(\"complete\").format(\"console\") \\\n",
        "      .queryName(\"counts\").start()\n",
        "\n",
        "# Wait until we terminate the scripts\n",
        "query.awaitTermination()\n",
        "\n",
        "# Stop the session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "ebNtxoTOgYSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}